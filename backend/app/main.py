"""
NewAPI Middleware Tool - FastAPI Backend
Main application entry point with CORS, logging, and exception handling.
"""
import asyncio
import logging
import threading
import time
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Any

from dotenv import load_dotenv

# Load environment variables from .env file in project root
env_path = Path(__file__).resolve().parent.parent.parent / ".env"
load_dotenv(env_path)

from fastapi import FastAPI, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from .logger import logger

# Suppress noisy loggers
logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
logging.getLogger("uvicorn.error").setLevel(logging.WARNING)


class ErrorResponse(BaseModel):
    """Standard error response format."""
    success: bool = False
    error: dict[str, Any]


class HealthResponse(BaseModel):
    """Health check response."""
    status: str
    version: str


# Custom exceptions
class AppException(Exception):
    """Base application exception."""
    def __init__(self, code: str, message: str, status_code: int = 500, details: Any = None):
        self.code = code
        self.message = message
        self.status_code = status_code
        self.details = details
        super().__init__(message)


class ContainerNotFoundError(AppException):
    """Raised when NewAPI container is not found."""
    def __init__(self, message: str = "NewAPI container not found"):
        super().__init__(
            code="CONTAINER_NOT_FOUND",
            message=message,
            status_code=503
        )


class DatabaseConnectionError(AppException):
    """Raised when database connection fails."""
    def __init__(self, message: str = "Database connection failed", details: Any = None):
        # Build descriptive error message with connection details
        if details:
            connection_info = []
            if "engine" in details:
                connection_info.append(f"engine={details['engine']}")
            if "host" in details:
                connection_info.append(f"host={details['host']}")
            if "port" in details:
                connection_info.append(f"port={details['port']}")
            if "database" in details:
                connection_info.append(f"database={details['database']}")
            if connection_info:
                message = f"{message} ({', '.join(connection_info)})"
        
        super().__init__(
            code="DB_CONNECTION_FAILED",
            message=message,
            status_code=503,
            details=details
        )


class InvalidParamsError(AppException):
    """Raised when request parameters are invalid."""
    def __init__(self, message: str = "Invalid parameters", details: Any = None):
        super().__init__(
            code="INVALID_PARAMS",
            message=message,
            status_code=400,
            details=details
        )


class UnauthorizedError(AppException):
    """Raised when API key is invalid."""
    def __init__(self, message: str = "Unauthorized"):
        super().__init__(
            code="UNAUTHORIZED",
            message=message,
            status_code=401
        )


class NotFoundError(AppException):
    """Raised when resource is not found."""
    def __init__(self, message: str = "Resource not found"):
        super().__init__(
            code="NOT_FOUND",
            message=message,
            status_code=404
        )


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    logger.system("NewAPI Middleware Tool å¯åŠ¨ä¸­...")

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    db = None
    index_status = {"all_ready": True}  # é»˜è®¤å€¼ï¼Œé˜²æ­¢æ•°æ®åº“è¿æ¥å¤±è´¥æ—¶æœªå®šä¹‰
    try:
        from .database import get_db_manager
        db = get_db_manager()
        db.connect()
        logger.system(f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {db.config.engine.value} @ {db.config.host}:{db.config.port}")
        
        # æ£€æŸ¥å¹¶æ¸…ç†å†—ä½™ç´¢å¼•ï¼Œç„¶åæ£€æŸ¥ç´¢å¼•çŠ¶æ€
        try:
            # å…ˆåˆ†æç´¢å¼•æƒ…å†µ
            analysis = db.get_logs_index_analysis()
            system_count = analysis.get('system_count', 0)
            ours_count = analysis.get('ours_count', 0)
            redundant_count = analysis.get('redundant_count', 0)
            unknown_count = analysis.get('unknown_count', 0)
            
            # å§‹ç»ˆè¾“å‡ºç´¢å¼•åˆ†æç»“æœ
            logger.system(f"Logsè¡¨ç´¢å¼•åˆ†æ: ç³»ç»Ÿ={system_count}, å·¥å…·={ours_count}, å†—ä½™={redundant_count}, æœªçŸ¥={unknown_count}")
            
            if redundant_count > 0:
                redundant_list = analysis.get('details', {}).get('redundant', [])
                logger.system(f"å‘ç° {redundant_count} ä¸ªå†—ä½™ç´¢å¼•: {redundant_list}ï¼Œå¼€å§‹æ¸…ç†...")
                cleanup_result = db.cleanup_redundant_indexes(log_progress=True)
                deleted = cleanup_result.get("deleted", 0)
                if deleted > 0:
                    logger.system(f"å·²æ¸…ç† {deleted} ä¸ªå†—ä½™ç´¢å¼•: {cleanup_result.get('deleted_indexes', [])}")
                else:
                    logger.system(f"å†—ä½™ç´¢å¼•æ¸…ç†å®Œæˆï¼Œæ— éœ€åˆ é™¤")
        except Exception as e:
            logger.warning(f"ç´¢å¼•åˆ†æ/æ¸…ç†å¤±è´¥: {e}", category="æ•°æ®åº“")
        
        # æ£€æŸ¥ç´¢å¼•çŠ¶æ€å¹¶è¾“å‡º
        index_status = db.get_index_status()
        if index_status["all_ready"]:
            logger.system(f"ç´¢å¼•æ£€æŸ¥å®Œæˆ: {index_status['existing']}/{index_status['total']} ä¸ªç´¢å¼•å·²å°±ç»ª")
        else:
            logger.system(f"ç´¢å¼•çŠ¶æ€: {index_status['existing']}/{index_status['total']} å·²å­˜åœ¨ï¼Œ{index_status['missing']} ä¸ªå¾…åˆ›å»º")
        
        # æ£€æµ‹ç³»ç»Ÿè§„æ¨¡
        try:
            from .system_scale_service import get_scale_service
            service = get_scale_service()
            result = service.detect_scale()
            metrics = result.get("metrics", {})
            settings = result.get("settings", {})
            logger.stats_box(f"ç³»ç»Ÿè§„æ¨¡: {settings.get('description', 'æœªçŸ¥')}", {
                "æ€»ç”¨æˆ·": metrics.get('total_users', 0),
                "24hæ´»è·ƒ": metrics.get('active_users_24h', 0),
                "24hæ—¥å¿—": metrics.get('logs_24h', 0),
                "RPM": f"{metrics.get('rpm_avg', 0):.1f}",
                "åˆ·æ–°é—´éš”": f"{settings.get('frontend_refresh_interval', 60)}s",
            })
        except Exception as e:
            logger.fail(f"ç³»ç»Ÿè§„æ¨¡æ£€æµ‹å¤±è´¥", error=str(e))
    except Exception as e:
        logger.warning(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}", category="æ•°æ®åº“")

    # å¯åŠ¨åå°ç´¢å¼•åˆ›å»ºä»»åŠ¡ï¼ˆä»…å½“æœ‰ç¼ºå¤±ç´¢å¼•æ—¶ï¼‰
    global _indexes_ready
    index_task = None
    if db and not index_status.get("all_ready", True):
        _indexes_ready = False  # æœ‰ç´¢å¼•éœ€è¦åˆ›å»ºï¼Œæ ‡è®°ä¸ºæœªå°±ç»ª
        index_task = asyncio.create_task(background_ensure_indexes())

    # å¯åŠ¨åå°ç¼“å­˜é¢„çƒ­ä»»åŠ¡ï¼ˆé¢„çƒ­å®Œæˆåä¼šå¯åŠ¨æ—¥å¿—åŒæ­¥ä»»åŠ¡å’Œ AI å°ç¦ä»»åŠ¡ï¼‰
    cache_warmup_task = asyncio.create_task(background_cache_warmup())

    # å¯åŠ¨ GeoIP æ•°æ®åº“è‡ªåŠ¨æ›´æ–°ä»»åŠ¡
    geoip_update_task = asyncio.create_task(background_geoip_update())

    # å¯åŠ¨ IP è®°å½•å¼ºåˆ¶å¼€å¯ä»»åŠ¡
    ip_recording_task = asyncio.create_task(background_enforce_ip_recording())

    yield

    # åœæ­¢åå°ä»»åŠ¡
    cache_warmup_task.cancel()
    geoip_update_task.cancel()
    ip_recording_task.cancel()
    if index_task:
        index_task.cancel()
    try:
        await cache_warmup_task
    except asyncio.CancelledError:
        pass
    try:
        await ip_recording_task
    except asyncio.CancelledError:
        pass
    if index_task:
        try:
            await index_task
        except asyncio.CancelledError:
            pass
    logger.system("NewAPI Middleware Tool å·²å…³é—­")


async def background_ensure_indexes():
    """
    Background task to create missing indexes without blocking app startup.
    Creates indexes one by one with delays to minimize database load.
    """
    global _indexes_ready
    
    # Wait a bit for app to fully start
    await asyncio.sleep(5)
    
    try:
        from .database import get_db_manager
        db = get_db_manager()
        
        logger.system("å¼€å§‹åå°åˆ›å»ºç¼ºå¤±ç´¢å¼•...")
        
        # Run index creation in thread pool to avoid blocking event loop
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, db.ensure_indexes_async_safe)
        
        logger.system("åå°ç´¢å¼•åˆ›å»ºå®Œæˆ")
    except asyncio.CancelledError:
        logger.system("ç´¢å¼•åˆ›å»ºä»»åŠ¡å·²å–æ¶ˆ")
    except Exception as e:
        logger.warning(f"åå°ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}", category="æ•°æ®åº“")
    finally:
        # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œéƒ½æ ‡è®°ç´¢å¼•ä»»åŠ¡å®Œæˆï¼Œè®©é¢„çƒ­ç»§ç»­
        _indexes_ready = True


# ç´¢å¼•åˆ›å»ºå®Œæˆæ ‡å¿—
_indexes_ready = True  # é»˜è®¤ Trueï¼Œå¦‚æœæ²¡æœ‰ç´¢å¼•ä»»åŠ¡åˆ™ç›´æ¥å°±ç»ª


async def background_enforce_ip_recording():
    """
    åå°ä»»åŠ¡ï¼šæ¯ 30 åˆ†é’Ÿæ£€æŸ¥å¹¶å¼ºåˆ¶å¼€å¯æ‰€æœ‰ç”¨æˆ·çš„ IP è®°å½•åŠŸèƒ½ã€‚
    é˜²æ­¢ç”¨æˆ·è‡ªè¡Œå…³é—­ IP è®°å½•å¯¼è‡´é£æ§æ•°æ®ç¼ºå¤±ã€‚
    """
    from .ip_monitoring_service import get_ip_monitoring_service

    # å¯åŠ¨åç­‰å¾… 60 ç§’å†å¼€å§‹æ£€æŸ¥
    await asyncio.sleep(60)
    logger.system("IP è®°å½•å¼ºåˆ¶å¼€å¯ä»»åŠ¡å·²å¯åŠ¨")

    while True:
        try:
            service = get_ip_monitoring_service()
            
            # è·å–å½“å‰ IP è®°å½•çŠ¶æ€
            stats = service.get_ip_recording_stats()
            total_users = stats.get("total_users", 0)
            enabled_count = stats.get("enabled_count", 0)
            disabled_count = stats.get("disabled_count", 0)
            
            if disabled_count > 0:
                # æœ‰ç”¨æˆ·å…³é—­äº† IP è®°å½•ï¼Œå¼ºåˆ¶å¼€å¯
                logger.system(f"[IPè®°å½•] æ£€æµ‹åˆ° {disabled_count} ä¸ªç”¨æˆ·å…³é—­äº† IP è®°å½•ï¼Œæ­£åœ¨å¼ºåˆ¶å¼€å¯...")
                
                result = service.enable_all_ip_recording()
                updated = result.get("updated", 0)
                
                if updated > 0:
                    logger.system(f"[IPè®°å½•] å·²å¼ºåˆ¶å¼€å¯ {updated} ä¸ªç”¨æˆ·çš„ IP è®°å½•")
                else:
                    logger.debug("[IPè®°å½•] æ— éœ€æ›´æ–°")
            else:
                logger.debug(f"[IPè®°å½•] æ‰€æœ‰ç”¨æˆ· ({total_users}) å·²å¼€å¯ IP è®°å½•")

        except asyncio.CancelledError:
            logger.system("IP è®°å½•å¼ºåˆ¶å¼€å¯ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.warning(f"[IPè®°å½•] å¼ºåˆ¶å¼€å¯ä»»åŠ¡å¤±è´¥: {e}", category="ä»»åŠ¡")

        # æ¯ 30 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        await asyncio.sleep(30 * 60)


async def background_log_sync():
    """åå°å®šæ—¶åŒæ­¥æ—¥å¿—åˆ†ææ•°æ®"""
    from .log_analytics_service import get_log_analytics_service

    # é¢„çƒ­å®Œæˆåç«‹å³å¯åŠ¨
    logger.success("åå°æ—¥å¿—åŒæ­¥ä»»åŠ¡å·²å¯åŠ¨", category="ä»»åŠ¡")

    while True:
        try:
            service = get_log_analytics_service()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–åŒæ­¥ï¼Œæœªåˆå§‹åŒ–æ—¶è·³è¿‡è‡ªåŠ¨åŒæ­¥
            sync_status = service.get_sync_status()
            if sync_status.get("needs_initial_sync") or sync_status.get("is_initializing"):
                # æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è‡ªåŠ¨åŒæ­¥ï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨è§¦å‘
                await asyncio.sleep(300)
                continue

            # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
            service.check_and_auto_reset()

            # å¤„ç†æ–°æ—¥å¿—ï¼ˆæ¯æ¬¡æœ€å¤šå¤„ç† 5000 æ¡ï¼‰
            total_processed = 0
            for _ in range(5):  # æœ€å¤š 5 è½®ï¼Œæ¯è½® 1000 æ¡
                result = service.process_new_logs()
                if not result.get("success") or result.get("processed", 0) == 0:
                    break
                total_processed += result.get("processed", 0)

            if total_processed > 0:
                logger.analytics("åå°åŒæ­¥å®Œæˆ", processed=total_processed)

        except Exception as e:
            logger.error(f"åå°æ—¥å¿—åŒæ­¥å¤±è´¥: {e}", category="ä»»åŠ¡")

        # æ¯ 5 åˆ†é’ŸåŒæ­¥ä¸€æ¬¡
        await asyncio.sleep(300)


async def _warmup_dashboard_data():
    """
    é¢„çƒ­ Dashboard æ•°æ®ï¼Œé¿å…é¦–æ¬¡è®¿é—®æ—¶æ•°æ®åº“è¶…æ—¶ã€‚

    å¯¹äºå¤§å‹ç³»ç»Ÿï¼ˆåƒä¸‡çº§æ—¥å¿—ï¼‰ï¼Œç›´æ¥æŸ¥è¯¢å¯èƒ½éœ€è¦ 10-30 ç§’ã€‚
    åœ¨å¯åŠ¨æ—¶é¢„çƒ­å¯ä»¥ç¡®ä¿ç”¨æˆ·é¦–æ¬¡è®¿é—®æ—¶æ•°æ®å·²ç»ç¼“å­˜ã€‚

    å‰ç«¯ Dashboard é¦–æ¬¡åŠ è½½ä¼šå¹¶å‘è°ƒç”¨ï¼š
    1. /api/dashboard/overview?period=7d
    2. /api/dashboard/usage?period=7d
    3. /api/dashboard/models?period=7d&limit=8
    4. /api/dashboard/trends/daily?days=7
    5. /api/dashboard/top-users?period=7d&limit=10  <-- å…³é”®ï¼

    å¿…é¡»å…¨éƒ¨é¢„çƒ­ï¼Œå¦åˆ™é¦–æ¬¡è®¿é—®ä¼šå¯¼è‡´é«˜å¹¶å‘æ•°æ®åº“æŸ¥è¯¢ã€‚

    ç¼“å­˜å­˜å‚¨ï¼šä½¿ç”¨ CacheManager ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ï¼ˆSQLite + Redis æ··åˆï¼‰

    å¢é‡ç¼“å­˜ï¼šå¯¹äº 3d/7d/14d å‘¨æœŸçš„ usage/models/top_usersï¼Œ
    ä½¿ç”¨æ§½ç¼“å­˜å¢é‡æ¨¡å¼ï¼ŒåªæŸ¥è¯¢ç¼ºå¤±çš„æ—¶é—´æ§½ï¼Œå¤§å¹…å‡å°‘æŸ¥è¯¢æ—¶é—´ã€‚
    """
    from .cached_dashboard import get_cached_dashboard_service, INCREMENTAL_PERIODS
    from .user_management_service import get_user_management_service
    from .system_scale_service import get_scale_service

    logger.phase(4, "é¢„çƒ­ Dashboard æ•°æ®")
    warmup_start = time.time()

    dashboard_service = get_cached_dashboard_service()
    user_service = get_user_management_service()

    # è·å–ç³»ç»Ÿè§„æ¨¡ä¿¡æ¯ç”¨äºä¼°ç®—
    try:
        scale_service = get_scale_service()
        scale_result = scale_service.detect_scale()
        metrics = scale_result.get("metrics", {})
        logs_24h = metrics.get('logs_24h', 0)
        total_logs = metrics.get('total_logs', 0)
    except:
        logs_24h = 0
        total_logs = 0

    # é¢„çƒ­é¡¹ç›®åˆ—è¡¨ï¼ˆæŒ‰å‰ç«¯åŠ è½½é¡ºåºï¼‰
    # å¿…é¡»åŒ…å«æ‰€æœ‰ Dashboard é¦–æ¬¡åŠ è½½æ—¶è°ƒç”¨çš„ API
    # æ ¼å¼: (name, period, method, kwargs, estimated_logs_multiplier)
    # - period: ç”¨äºåˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¢é‡ç¼“å­˜ï¼ˆ3d/7d/14dï¼‰
    # - method: æ–¹æ³•å
    # - kwargs: æ–¹æ³•å‚æ•°
    # - multiplier: åŸºäº 24h æ—¥å¿—æ•°ä¼°ç®—æ‰«æé‡
    warmup_items = [
        # === æ ¸å¿ƒ Dashboard APIï¼ˆå‰ç«¯ Promise.all å¹¶å‘è°ƒç”¨ï¼‰===
        ("overview_7d", "7d", "get_system_overview", {"period": "7d"}, 5.0),
        ("usage_7d", "7d", "get_usage_statistics", {"period": "7d"}, 5.0),
        ("models_7d", "7d", "get_model_usage", {"period": "7d", "limit": 8}, 5.0),
        ("trends_daily_7d", None, "get_daily_trends", {"days": 7}, 5.0),
        ("top_users_7d", "7d", "get_top_users", {"period": "7d", "limit": 10}, 5.0),

        # === å¸¸ç”¨çš„å…¶ä»–æ—¶é—´å‘¨æœŸ ===
        ("overview_24h", "24h", "get_system_overview", {"period": "24h"}, 1.0),
        ("usage_24h", "24h", "get_usage_statistics", {"period": "24h"}, 1.0),
        ("trends_hourly_24h", None, "get_hourly_trends", {"hours": 24}, 1.0),

        # === 3å¤©å‘¨æœŸï¼ˆç”¨æˆ·åˆ‡æ¢æ—¶é—´å‘¨æœŸæ—¶éœ€è¦ï¼‰===
        ("overview_3d", "3d", "get_system_overview", {"period": "3d"}, 2.5),
        ("usage_3d", "3d", "get_usage_statistics", {"period": "3d"}, 2.5),
        ("models_3d", "3d", "get_model_usage", {"period": "3d", "limit": 8}, 2.5),
        ("trends_daily_3d", None, "get_daily_trends", {"days": 3}, 2.5),
        ("top_users_3d", "3d", "get_top_users", {"period": "3d", "limit": 10}, 2.5),

        # === 14å¤©å‘¨æœŸï¼ˆç”¨æˆ·åˆ‡æ¢æ—¶é—´å‘¨æœŸæ—¶éœ€è¦ï¼‰===
        ("overview_14d", "14d", "get_system_overview", {"period": "14d"}, 10.0),
        ("usage_14d", "14d", "get_usage_statistics", {"period": "14d"}, 10.0),
        ("models_14d", "14d", "get_model_usage", {"period": "14d", "limit": 8}, 10.0),
        ("trends_daily_14d", None, "get_daily_trends", {"days": 14}, 10.0),
        ("top_users_14d", "14d", "get_top_users", {"period": "14d", "limit": 10}, 10.0),

        # === ç”¨æˆ·ç»Ÿè®¡ï¼ˆUserManagement é¡µé¢éœ€è¦ï¼‰===
        ("user_stats", None, None, {}, 1.0),
    ]

    # è®¡ç®—é¢„è®¡æ‰«ææ€»æ—¥å¿—æ•°
    total_estimated_logs = sum(int(logs_24h * m) for _, _, _, _, m in warmup_items)

    # ç»Ÿè®¡å¢é‡ç¼“å­˜é¡¹ç›®æ•°
    incremental_count = sum(
        1 for _, period, method, _, _ in warmup_items
        if period in INCREMENTAL_PERIODS and method in ("get_usage_statistics", "get_model_usage", "get_top_users")
    )

    logger.kvs({
        "å¾…é¢„çƒ­é¡¹ç›®": f"{len(warmup_items)} ä¸ª",
        "å¢é‡ç¼“å­˜é¡¹ç›®": f"{incremental_count} ä¸ª",
        "é¢„è®¡æ‰«ææ—¥å¿—": f"{total_estimated_logs:,} æ¡",
    })

    total_items = len(warmup_items)
    success_count = 0
    failed_items = []

    for idx, (name, period, method, kwargs, multiplier) in enumerate(warmup_items):
        estimated_logs = int(logs_24h * multiplier)

        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¢é‡ç¼“å­˜ï¼ˆä»… usage/models/top_users æ”¯æŒï¼‰
        is_incremental = (
            period in INCREMENTAL_PERIODS and
            method in ("get_usage_statistics", "get_model_usage", "get_top_users")
        )
        mode_tag = " [å¢é‡]" if is_incremental else ""

        try:
            item_start = time.time()

            # æ„å»ºè°ƒç”¨å‚æ•°
            call_kwargs = {"use_cache": False, **kwargs}
            if is_incremental:
                call_kwargs["log_progress"] = True

            # è·å–è¦è°ƒç”¨çš„æ–¹æ³•
            if method is None:
                # user_stats ç‰¹æ®Šå¤„ç†
                fetch_func = lambda: user_service.get_activity_stats()
            else:
                service_method = getattr(dashboard_service, method)
                # ä½¿ç”¨é—­åŒ…æ•è·å½“å‰å‚æ•°
                fetch_func = lambda m=service_method, k=call_kwargs: m(**k)

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            await asyncio.get_event_loop().run_in_executor(None, fetch_func)
            item_elapsed = time.time() - item_start
            logger.success(f"Dashboard {name}{mode_tag} é¢„çƒ­å®Œæˆ", è€—æ—¶=f"{item_elapsed:.2f}s")
            success_count += 1
        except Exception as e:
            failed_items.append(name)
            logger.warn(f"Dashboard {name}{mode_tag} é¢„çƒ­å¤±è´¥: {e}")

    total_elapsed = time.time() - warmup_start

    # è¾“å‡ºæ±‡æ€»ï¼ˆä¸æ’è¡Œæ¦œé¢„çƒ­æ ¼å¼ä¸€è‡´ï¼‰
    if failed_items:
        logger.kvs({
            "æˆåŠŸé¡¹ç›®": f"{success_count}/{total_items}",
            "å¤±è´¥é¡¹ç›®": ", ".join(failed_items),
            "æ€»è€—æ—¶": f"{total_elapsed:.1f}s",
        })
    else:
        logger.kvs({
            "æˆåŠŸé¡¹ç›®": f"{success_count}/{total_items}",
            "æ€»è€—æ—¶": f"{total_elapsed:.1f}s",
        })


async def _warmup_user_activity_list():
    """
    é¢„çƒ­ç”¨æˆ·ç®¡ç†æ´»è·ƒåº¦ç­›é€‰æ•°æ®ï¼ˆä»…å¤§å‹/è¶…å¤§å‹ç³»ç»Ÿï¼‰ã€‚

    å¯¹äºå¤§å‹ç³»ç»Ÿï¼Œæ´»è·ƒåº¦ç­›é€‰éœ€è¦ JOIN logs è¡¨ï¼Œé¦–æ¬¡æŸ¥è¯¢å¯èƒ½éœ€è¦ 10-30 ç§’ã€‚
    é¢„çƒ­å¯ä»¥ç¡®ä¿ç”¨æˆ·é¦–æ¬¡è®¿é—®ç”¨æˆ·ç®¡ç†é¡µé¢æ—¶æ•°æ®å·²ç»ç¼“å­˜ã€‚

    é¢„çƒ­å†…å®¹ï¼š
    - activeï¼ˆæ´»è·ƒç”¨æˆ·ï¼‰ç¬¬1é¡µ
    - inactiveï¼ˆä¸æ´»è·ƒç”¨æˆ·ï¼‰ç¬¬1é¡µ
    - very_inactiveï¼ˆéå¸¸ä¸æ´»è·ƒç”¨æˆ·ï¼‰ç¬¬1é¡µ

    å°å‹/ä¸­å‹ç³»ç»Ÿè·³è¿‡æ­¤é¢„çƒ­ï¼ˆæŸ¥è¯¢æœ¬èº«è¾ƒå¿«ï¼‰ã€‚
    """
    from .system_scale_service import get_scale_service
    from .user_management_service import get_user_management_service, ActivityLevel

    # æ£€æŸ¥ç³»ç»Ÿè§„æ¨¡
    try:
        scale_service = get_scale_service()
        scale_result = scale_service.detect_scale()
        scale = scale_result.get("scale", "medium")
    except Exception:
        scale = "medium"

    # åªæœ‰å¤§å‹/è¶…å¤§å‹ç³»ç»Ÿæ‰é¢„çƒ­
    if scale not in ("large", "xlarge"):
        logger.bullet(f"ç”¨æˆ·æ´»è·ƒåº¦åˆ—è¡¨ï¼šè·³è¿‡é¢„çƒ­ï¼ˆç³»ç»Ÿè§„æ¨¡={scale}ï¼Œæ— éœ€é¢„çƒ­ï¼‰")
        return

    logger.phase(5, "é¢„çƒ­ç”¨æˆ·æ´»è·ƒåº¦åˆ—è¡¨ï¼ˆå¤§å‹ç³»ç»Ÿï¼‰")
    warmup_start = time.time()

    user_service = get_user_management_service()

    # é¢„çƒ­é¡¹ç›®ï¼š3ç§æ´»è·ƒåº¦ç­›é€‰çš„ç¬¬1é¡µ
    warmup_items = [
        ("active", ActivityLevel.ACTIVE),
        ("inactive", ActivityLevel.INACTIVE),
        ("very_inactive", ActivityLevel.VERY_INACTIVE),
    ]

    success_count = 0
    failed_items = []

    for name, activity_filter in warmup_items:
        try:
            item_start = time.time()
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            await asyncio.get_event_loop().run_in_executor(
                None,
                lambda af=activity_filter: user_service.get_users(
                    page=1,
                    page_size=20,
                    activity_filter=af,
                    order_by="last_request_time",
                    order_dir="DESC",
                )
            )
            item_elapsed = time.time() - item_start
            logger.success(f"ç”¨æˆ·æ´»è·ƒåº¦ {name} é¢„çƒ­å®Œæˆ", è€—æ—¶=f"{item_elapsed:.2f}s")
            success_count += 1
        except Exception as e:
            failed_items.append(name)
            logger.warn(f"ç”¨æˆ·æ´»è·ƒåº¦ {name} é¢„çƒ­å¤±è´¥: {e}")

        # æ¯ä¸ªæŸ¥è¯¢ä¹‹é—´å»¶è¿Ÿ 1 ç§’ï¼Œé¿å…æ•°æ®åº“å‹åŠ›
        await asyncio.sleep(1)

    total_elapsed = time.time() - warmup_start

    if failed_items:
        logger.kvs({
            "æˆåŠŸé¡¹ç›®": f"{success_count}/{len(warmup_items)}",
            "å¤±è´¥é¡¹ç›®": ", ".join(failed_items),
            "æ€»è€—æ—¶": f"{total_elapsed:.1f}s",
        })
    else:
        logger.kvs({
            "æˆåŠŸé¡¹ç›®": f"{success_count}/{len(warmup_items)}",
            "æ€»è€—æ—¶": f"{total_elapsed:.1f}s",
        })


async def _warmup_ip_monitoring_data():
    """
    é¢„çƒ­ IP ç›‘æ§æ•°æ®ï¼ˆå…±äº«IPã€å¤šIPä»¤ç‰Œã€å¤šIPç”¨æˆ·ã€IP Statsï¼‰

    ç‰¹ç‚¹ï¼š
    1. é¢„çƒ­å¤šä¸ªæ—¶é—´çª—å£ï¼ˆ1h, 24h, 7dï¼‰
    2. ä½¿ç”¨ limit=200 åŒ¹é…å‰ç«¯è¯·æ±‚
    3. æ”¯æŒç¼“å­˜å¤ç”¨ï¼ˆkeyä¸åŒ…å«limitï¼‰
    """
    from .ip_monitoring_service import get_ip_monitoring_service, WINDOW_SECONDS
    from .cache_manager import get_cache_manager

    logger.phase(5, "é¢„çƒ­ IP ç›‘æ§æ•°æ®")

    ip_service = get_ip_monitoring_service()
    cache = get_cache_manager()
    IP_WARMUP_LIMIT = 200
    IP_WARMUP_WINDOWS = ["1h", "24h", "7d"]

    warmup_start = time.time()
    success_count = 0
    failed_items = []

    for window_key in IP_WARMUP_WINDOWS:
        window_seconds = WINDOW_SECONDS.get(window_key, 86400)

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¢é‡ç¼“å­˜ï¼ˆ3d/7dï¼‰
        is_incremental = cache.is_incremental_window(window_key)
        mode_tag = " [å¢é‡]" if is_incremental else ""

        # å…±äº«IP
        try:
            start = time.time()
            ip_service.get_shared_ips(
                window_seconds=window_seconds,
                min_tokens=2,
                limit=IP_WARMUP_LIMIT,
                use_cache=False,
                log_progress=is_incremental,  # å¢é‡æ¨¡å¼æ—¶æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            )
            logger.success(f"IPç›‘æ§ shared_ips({window_key}){mode_tag} é¢„çƒ­å®Œæˆ", è€—æ—¶=f"{time.time()-start:.2f}s")
            success_count += 1
        except Exception as e:
            failed_items.append(f"shared_ips({window_key})")
            logger.warn(f"IPç›‘æ§ shared_ips({window_key}) é¢„çƒ­å¤±è´¥: {e}")

        # å¤šIPä»¤ç‰Œ
        try:
            start = time.time()
            ip_service.get_multi_ip_tokens(
                window_seconds=window_seconds,
                min_ips=2,
                limit=IP_WARMUP_LIMIT,
                use_cache=False,
                log_progress=is_incremental,
            )
            logger.success(f"IPç›‘æ§ multi_ip_tokens({window_key}){mode_tag} é¢„çƒ­å®Œæˆ", è€—æ—¶=f"{time.time()-start:.2f}s")
            success_count += 1
        except Exception as e:
            failed_items.append(f"multi_ip_tokens({window_key})")
            logger.warn(f"IPç›‘æ§ multi_ip_tokens({window_key}) é¢„çƒ­å¤±è´¥: {e}")

        # å¤šIPç”¨æˆ·
        try:
            start = time.time()
            ip_service.get_multi_ip_users(
                window_seconds=window_seconds,
                min_ips=3,
                limit=IP_WARMUP_LIMIT,
                use_cache=False,
                log_progress=is_incremental,
            )
            logger.success(f"IPç›‘æ§ multi_ip_users({window_key}){mode_tag} é¢„çƒ­å®Œæˆ", è€—æ—¶=f"{time.time()-start:.2f}s")
            success_count += 1
        except Exception as e:
            failed_items.append(f"multi_ip_users({window_key})")
            logger.warn(f"IPç›‘æ§ multi_ip_users({window_key}) é¢„çƒ­å¤±è´¥: {e}")

    # IP Stats
    try:
        start = time.time()
        ip_service.get_ip_recording_stats(use_cache=False)
        logger.success(f"IPç›‘æ§ ip_stats é¢„çƒ­å®Œæˆ", è€—æ—¶=f"{time.time()-start:.2f}s")
        success_count += 1
    except Exception as e:
        failed_items.append("ip_stats")
        logger.warn(f"IPç›‘æ§ ip_stats é¢„çƒ­å¤±è´¥: {e}")

    total_elapsed = time.time() - warmup_start
    total_items = len(IP_WARMUP_WINDOWS) * 3 + 1  # 3ç§æŸ¥è¯¢ Ã— Nçª—å£ + IP Stats

    if failed_items:
        logger.kvs({
            "æˆåŠŸé¡¹ç›®": f"{success_count}/{total_items}",
            "å¤±è´¥é¡¹ç›®": ", ".join(failed_items),
            "æ€»è€—æ—¶": f"{total_elapsed:.1f}s",
        })
    else:
        logger.kvs({
            "æˆåŠŸé¡¹ç›®": f"{success_count}/{total_items}",
            "æ€»è€—æ—¶": f"{total_elapsed:.1f}s",
        })


async def background_cache_warmup():
    """
    åå°ç¼“å­˜é¢„çƒ­ä»»åŠ¡ - æ™ºèƒ½æ¢å¤æ¨¡å¼
    
    æ–°ç­–ç•¥ï¼š
    1. ç­‰å¾…ç´¢å¼•åˆ›å»ºå®Œæˆï¼ˆå¦‚æœæœ‰ï¼‰
    2. ä¼˜å…ˆä» SQLite æ¢å¤ç¼“å­˜ï¼ˆç§’çº§æ¢å¤ï¼‰
    3. ä»…ç¼ºå¤±çš„çª—å£æ‰æŸ¥è¯¢ PostgreSQL
    4. æ¢å¤åè¿›å…¥å®šæ—¶åˆ·æ–°å¾ªç¯
    
    åƒä¸‡çº§æ•°æ®å¤„ç†ç­–ç•¥ï¼š
    ========================
    æœ¬ç³»ç»Ÿä¸ä¼šåŠ è½½å…¨é‡æ•°æ®åˆ°å†…å­˜ï¼Œè€Œæ˜¯é‡‡ç”¨ä»¥ä¸‹ä¼˜åŒ–ç­–ç•¥ï¼š
    
    1. èšåˆæŸ¥è¯¢ï¼šSQL ä½¿ç”¨ GROUP BY user_id èšåˆï¼Œåªè¿”å› Top 50 ç”¨æˆ·
       - å³ä½¿æœ‰ 1000 ä¸‡æ¡æ—¥å¿—ï¼Œä¹Ÿåªè¿”å› 50 æ¡èšåˆç»“æœ
       - æ•°æ®åº“åœ¨æœåŠ¡ç«¯å®Œæˆèšåˆï¼Œä¸ä¼ è¾“åŸå§‹æ•°æ®
    
    2. ç´¢å¼•ä¼˜åŒ–ï¼šä½¿ç”¨å¤åˆç´¢å¼• idx_logs_created_type_user
       - ç´¢å¼•è¦†ç›– (created_at, type, user_id)
       - æŸ¥è¯¢åªæ‰«æç´¢å¼•ï¼Œä¸å›è¡¨è¯»å–å…¨éƒ¨å­—æ®µ
    
    3. æ—¶é—´çª—å£ï¼šæŒ‰æ—¶é—´çª—å£åˆ†åˆ«ç¼“å­˜ (1h/3h/6h/12h/24h/3d/7d)
       - æ¯ä¸ªçª—å£ç‹¬ç«‹ç¼“å­˜ï¼Œç‹¬ç«‹åˆ·æ–°
       - çŸ­çª—å£æ•°æ®é‡å°ï¼ŒæŸ¥è¯¢å¿«
    
    4. ä¸‰å±‚ç¼“å­˜ï¼šRedis â†’ SQLite â†’ PostgreSQL
       - çƒ­æ•°æ®åœ¨ Redisï¼ˆæ¯«ç§’çº§å“åº”ï¼‰
       - æŒä¹…åŒ–åˆ° SQLiteï¼ˆé‡å¯åç§’çº§æ¢å¤ï¼‰
       - åªæœ‰ç¼“å­˜å¤±æ•ˆæ‰æŸ¥è¯¢ PostgreSQL
    
    5. å»¶è¿Ÿç­–ç•¥ï¼šæ ¹æ®ç³»ç»Ÿè§„æ¨¡è°ƒæ•´æŸ¥è¯¢é—´éš”
       - å°å‹ç³»ç»Ÿï¼šæ— å»¶è¿Ÿ
       - ä¸­å‹ç³»ç»Ÿï¼š0.5s å»¶è¿Ÿ
       - å¤§å‹ç³»ç»Ÿï¼š1s å»¶è¿Ÿ
       - è¶…å¤§å‹ç³»ç»Ÿï¼š2s å»¶è¿Ÿ
    """
    from .system_scale_service import get_detected_settings, get_scale_service, SystemScale
    from .database import get_db_manager
    from .cache_manager import get_cache_manager

    warmup_start_time = time.time()

    # å¯åŠ¨åç­‰å¾… 3 ç§’ï¼Œè®©æ•°æ®åº“è¿æ¥å°±ç»ª
    await asyncio.sleep(3)
    
    # ç­‰å¾…ç´¢å¼•åˆ›å»ºå®Œæˆï¼ˆæœ€å¤šç­‰å¾… 10 åˆ†é’Ÿï¼‰
    global _indexes_ready
    if not _indexes_ready:
        logger.system("ç­‰å¾…ç´¢å¼•åˆ›å»ºå®Œæˆåå†å¼€å§‹é¢„çƒ­...")
        _set_warmup_status("initializing", 0, "ç­‰å¾…ç´¢å¼•åˆ›å»ºå®Œæˆ...")
        
        wait_count = 0
        max_wait = 600  # æœ€å¤šç­‰å¾… 600 ç§’ï¼ˆ10 åˆ†é’Ÿï¼‰
        while not _indexes_ready and wait_count < max_wait:
            await asyncio.sleep(5)
            wait_count += 5
        
        if _indexes_ready:
            logger.system("ç´¢å¼•åˆ›å»ºå®Œæˆï¼Œå¼€å§‹é¢„çƒ­")
        else:
            logger.warning("ç´¢å¼•åˆ›å»ºè¶…æ—¶ï¼Œç»§ç»­é¢„çƒ­ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰")
    
    logger.banner("ğŸš€ ç¼“å­˜æ¢å¤ä»»åŠ¡å¯åŠ¨")

    # åˆå§‹åŒ–é¢„çƒ­æ­¥éª¤
    steps = [
        {"name": "æ¢å¤ç¼“å­˜", "status": "pending"},
        {"name": "æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§", "status": "pending"},
        {"name": "é¢„çƒ­æ’è¡Œæ¦œæ•°æ®", "status": "pending"},
        {"name": "é¢„çƒ­ Dashboard", "status": "pending"},
        {"name": "é¢„çƒ­ç”¨æˆ·æ´»è·ƒåº¦", "status": "pending"},
        {"name": "é¢„çƒ­ IP ç›‘æ§", "status": "pending"},
        {"name": "é¢„çƒ­ IP åˆ†å¸ƒ", "status": "pending"},
        {"name": "é¢„çƒ­æ¨¡å‹çŠ¶æ€", "status": "pending"},
    ]

    _set_warmup_status("initializing", 0, "æ­£åœ¨åˆå§‹åŒ–ç¼“å­˜...", steps)

    # è·å–ç¼“å­˜ç®¡ç†å™¨
    cache = get_cache_manager()
    
    # é˜¶æ®µ1ï¼šä» SQLite æ¢å¤åˆ° Redisï¼ˆå¦‚æœ Redis å¯ç”¨ï¼‰
    logger.phase(1, "ä» SQLite æ¢å¤ç¼“å­˜åˆ° Redis")
    steps[0]["status"] = "done"
    _set_warmup_status("initializing", 5, "æ­£åœ¨æ¢å¤ç¼“å­˜...", steps)
    if cache.redis_available:
        restored = cache.restore_to_redis()
        if restored > 0:
            logger.success(f"æ¢å¤å®Œæˆ", count=restored)
        else:
            logger.bullet("æ— ç¼“å­˜æ•°æ®éœ€è¦æ¢å¤")
    else:
        logger.bullet("Redis æœªé…ç½®ï¼Œä½¿ç”¨çº¯ SQLite æ¨¡å¼")
    
    # é˜¶æ®µ2ï¼šæ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§
    logger.phase(2, "æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§")
    steps[1]["status"] = "done"
    _set_warmup_status("initializing", 10, "æ­£åœ¨æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§...", steps)

    windows = ["1h", "3h", "6h", "12h", "24h", "3d", "7d"]
    cached_windows = cache.get_cached_windows()
    missing_windows = [w for w in windows if w not in cached_windows]

    if not missing_windows:
        # æ‰€æœ‰ç¼“å­˜éƒ½æœ‰æ•ˆï¼Œä½†ä»éœ€é¢„çƒ­ Dashboardã€IPç›‘æ§ å’Œ IP åˆ†å¸ƒ
        logger.success("æ‰€æœ‰ç¼“å­˜æœ‰æ•ˆï¼Œæ— éœ€é¢„çƒ­æ’è¡Œæ¦œ")
        steps[2]["status"] = "done"  # æ’è¡Œæ¦œè·³è¿‡
        _set_warmup_status("initializing", 40, "æ’è¡Œæ¦œç¼“å­˜æœ‰æ•ˆï¼Œæ­£åœ¨é¢„çƒ­ Dashboard...", steps)

        # é¢„çƒ­ Dashboard æ•°æ®
        steps[3]["status"] = "pending"
        try:
            await _warmup_dashboard_data()
            steps[3]["status"] = "done"
        except Exception as e:
            logger.warn(f"Dashboard é¢„çƒ­å¼‚å¸¸: {e}")
            steps[3]["status"] = "error"
        _set_warmup_status("initializing", 55, "æ­£åœ¨é¢„çƒ­ç”¨æˆ·æ´»è·ƒåº¦åˆ—è¡¨...", steps)

        # é¢„çƒ­ç”¨æˆ·æ´»è·ƒåº¦åˆ—è¡¨ï¼ˆä»…å¤§å‹ç³»ç»Ÿï¼‰
        try:
            await _warmup_user_activity_list()
            steps[4]["status"] = "done"
        except Exception as e:
            logger.warn(f"ç”¨æˆ·æ´»è·ƒåº¦åˆ—è¡¨é¢„çƒ­å¼‚å¸¸: {e}")
            steps[4]["status"] = "error"
        _set_warmup_status("initializing", 65, "æ­£åœ¨é¢„çƒ­ IP ç›‘æ§æ•°æ®...", steps)

        # é¢„çƒ­ IP ç›‘æ§æ•°æ®ï¼ˆå…±äº«IPã€å¤šIPä»¤ç‰Œã€å¤šIPç”¨æˆ·ã€IP Statsï¼‰
        try:
            await _warmup_ip_monitoring_data()
            steps[5]["status"] = "done"
        except Exception as e:
            logger.warn(f"IPç›‘æ§é¢„çƒ­å¼‚å¸¸: {e}")
            steps[5]["status"] = "error"
        _set_warmup_status("initializing", 80, "æ­£åœ¨é¢„çƒ­ IP åœ°åŒºåˆ†å¸ƒ...", steps)

        # é¢„çƒ­ IP åœ°åŒºåˆ†å¸ƒ
        try:
            from .ip_distribution_service import warmup_ip_distribution
            await warmup_ip_distribution()
            steps[6]["status"] = "done"
        except Exception as e:
            logger.warning(f"[IPåˆ†å¸ƒ] é¢„çƒ­å¼‚å¸¸: {e}")
            steps[6]["status"] = "error"
        _set_warmup_status("initializing", 90, "æ­£åœ¨é¢„çƒ­æ¨¡å‹çŠ¶æ€...", steps)

        # é¢„çƒ­æ¨¡å‹çŠ¶æ€ç›‘æ§æ•°æ®ï¼ˆåŠ¨æ€è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼‰
        try:
            from .model_status_service import warmup_model_status
            await warmup_model_status()
            steps[7]["status"] = "done"
        except Exception as e:
            logger.warning(f"[æ¨¡å‹çŠ¶æ€] é¢„çƒ­å¼‚å¸¸: {e}")
            steps[7]["status"] = "error"
        
        # æ‰€æœ‰é¢„çƒ­å®Œæˆ
        elapsed = time.time() - warmup_start_time
        _set_warmup_status("ready", 100, f"é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}s", steps)
        logger.banner("âœ… ç¼“å­˜é¢„çƒ­å®Œæˆ")
        logger.kvs({
            "æ€»è€—æ—¶": f"{elapsed:.1f}s",
        })
        
        # é¢„çƒ­å®Œæˆåå¯åŠ¨åå°ä»»åŠ¡
        asyncio.create_task(background_log_sync())
        asyncio.create_task(background_ai_auto_ban_scan())
        asyncio.create_task(background_auto_group_scan())
        
        # è¿›å…¥å®šæ—¶åˆ·æ–°å¾ªç¯
        await _background_refresh_loop(cache)
        return
    
    logger.bullet(f"å·²ç¼“å­˜: {cached_windows or 'æ— '}")
    logger.bullet(f"éœ€é¢„çƒ­: {missing_windows}")

    # æ£€æµ‹ç³»ç»Ÿè§„æ¨¡
    scale_service = get_scale_service()
    scale_result = scale_service.detect_scale()
    scale = SystemScale(scale_result["scale"])
    metrics = scale_result.get("metrics", {})

    # è¾“å‡ºç³»ç»Ÿè§„æ¨¡è¯¦æƒ…
    logger.stats_box("ç³»ç»Ÿè§„æ¨¡æ£€æµ‹", {
        "ç³»ç»Ÿè§„æ¨¡": scale.value,
        "æ€»ç”¨æˆ·æ•°": metrics.get('total_users', 0),
        "æ´»è·ƒç”¨æˆ·(24h)": metrics.get('active_users_24h', 0),
        "æ—¥å¿—æ•°(24h)": metrics.get('logs_24h', 0),
        "æ€»æ—¥å¿—æ•°": metrics.get('total_logs', 0),
        "å¹³å‡ RPM": f"{metrics.get('rpm_avg', 0):.1f}",
    })

    # è·å–é¢„çƒ­ç­–ç•¥
    strategy = WARMUP_STRATEGY.get(scale.value, WARMUP_STRATEGY["medium"])
    query_delay = strategy['query_delay']
    
    # ä¼°ç®—é¢„çƒ­æ—¶é—´å’Œæ•°æ®é‡
    total_to_warm = len(missing_windows)
    logs_24h = metrics.get('logs_24h', 0)
    total_logs = metrics.get('total_logs', 0)
    
    # æ ¹æ®ç³»ç»Ÿè§„æ¨¡ä¼°ç®—æ¯ä¸ªçª—å£çš„æŸ¥è¯¢æ—¶é—´
    if logs_24h > 5000000:  # 500ä¸‡+
        estimated_query_time = 5.0  # å¤§æ•°æ®é‡ï¼Œæ¯ä¸ªçª—å£çº¦5ç§’
    elif logs_24h > 1000000:  # 100ä¸‡+
        estimated_query_time = 3.0
    elif logs_24h > 100000:  # 10ä¸‡+
        estimated_query_time = 1.5
    else:
        estimated_query_time = 0.5
    
    # ä¼°ç®—æ¯ä¸ªçª—å£éœ€è¦æ‰«æçš„æ—¥å¿—æ•°é‡
    # åŸºäº 24h æ—¥å¿—æ•°æŒ‰æ¯”ä¾‹ä¼°ç®—
    window_logs_estimate = {}
    window_hours = {"1h": 1, "3h": 3, "6h": 6, "12h": 12, "24h": 24, "3d": 72, "7d": 168}
    hourly_rate = logs_24h / 24 if logs_24h > 0 else 0
    
    total_logs_to_scan = 0
    for w in missing_windows:
        hours = window_hours.get(w, 24)
        if hours <= 24:
            # 24å°æ—¶å†…ï¼ŒæŒ‰å°æ—¶æ¯”ä¾‹ä¼°ç®—
            estimated_logs = int(hourly_rate * hours)
        else:
            # è¶…è¿‡24å°æ—¶ï¼Œå‡è®¾å†å²æ•°æ®é‡é€’å‡ï¼ˆè¶Šæ—©çš„æ•°æ®è¶Šå°‘ï¼‰
            # ä½¿ç”¨ 24h æ•°æ® * ç³»æ•°ä¼°ç®—
            if w == "3d":
                estimated_logs = int(logs_24h * 2.5)  # 3å¤©çº¦ä¸º24hçš„2.5å€
            else:  # 7d
                estimated_logs = int(logs_24h * 5)  # 7å¤©çº¦ä¸º24hçš„5å€
        
        window_logs_estimate[w] = estimated_logs
        total_logs_to_scan += estimated_logs
    
    # æ€»é¢„è®¡æ—¶é—´ = (æŸ¥è¯¢æ—¶é—´ + å»¶è¿Ÿ) * çª—å£æ•°
    estimated_total_time = (estimated_query_time + query_delay) * total_to_warm
    
    # é¢„çƒ­æ•°æ®æ¡æ•°è¯´æ˜
    # æ¯ä¸ªçª—å£è¿”å› Top 50 ç”¨æˆ·çš„èšåˆæ•°æ®
    estimated_records = total_to_warm * 50

    # === é˜¶æ®µ3ï¼šä»…é¢„çƒ­ç¼ºå¤±çš„çª—å£ ===
    logger.phase(3, "é¢„çƒ­ç¼ºå¤±çš„çª—å£")
    logger.kvs({
        "å¾…é¢„çƒ­çª—å£": f"{total_to_warm} ä¸ª",
        "é¢„è®¡æ‰«ææ—¥å¿—": f"{total_logs_to_scan:,} æ¡",
        "é¢„è®¡ç¼“å­˜æ•°æ®": f"{estimated_records} æ¡",
        "æŸ¥è¯¢å»¶è¿Ÿ": f"{query_delay}s/çª—å£",
        "é¢„è®¡è€—æ—¶": f"{estimated_total_time:.0f}~{estimated_total_time * 1.5:.0f} ç§’",
    })
    _set_warmup_status("initializing", 15, f"æ­£åœ¨é¢„çƒ­æ’è¡Œæ¦œ ({total_to_warm} ä¸ªçª—å£)...", steps)

    from .risk_monitoring_service import get_risk_monitoring_service
    service = get_risk_monitoring_service()
    
    warmed = []
    failed = []
    window_times = []  # è®°å½•æ¯ä¸ªçª—å£çš„å®é™…è€—æ—¶ï¼Œç”¨äºåŠ¨æ€ä¼°ç®—
    
    for idx, window in enumerate(missing_windows):
        # progress: 15% -> 50% (æ’è¡Œæ¦œé¢„çƒ­å  35%)
        progress = 15 + int((idx / max(total_to_warm, 1)) * 35)
        
        # è·å–è¯¥çª—å£é¢„è®¡æ‰«æçš„æ—¥å¿—æ•°
        window_estimated_logs = window_logs_estimate.get(window, 0)
        
        # è®¡ç®—å‰©ä½™é¢„è®¡æ—¶é—´
        if window_times:
            avg_time = sum(window_times) / len(window_times)
            remaining_windows = total_to_warm - idx
            remaining_time = (avg_time + query_delay) * remaining_windows
            _set_warmup_status("initializing", progress, f"æ’è¡Œæ¦œ: {window} ({idx + 1}/{total_to_warm})ï¼Œå‰©ä½™çº¦ {remaining_time:.0f}s", steps)
        else:
            remaining_time = estimated_total_time - (estimated_query_time + query_delay) * idx
            _set_warmup_status("initializing", progress, f"æ’è¡Œæ¦œ: {window} ({idx + 1}/{total_to_warm})ï¼Œå‰©ä½™çº¦ {remaining_time:.0f}s", steps)
        
        window_start = time.time()

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¢é‡ç¼“å­˜ï¼ˆ3d/7dï¼‰
        is_incremental = cache.is_incremental_window(window)
        if is_incremental:
            # æ£€æŸ¥æ§½ç¼“å­˜çŠ¶æ€
            missing_slots, cached_slots = cache.get_missing_slots(window, "requests")
            total_slots = len(missing_slots) + len(cached_slots)
            if cached_slots:
                logger.step(idx + 1, total_to_warm, f"é¢„çƒ­ {window} çª—å£ [å¢é‡æ¨¡å¼: {len(cached_slots)}/{total_slots} æ§½å·²ç¼“å­˜]")
            else:
                logger.step(idx + 1, total_to_warm, f"é¢„çƒ­ {window} çª—å£ [å¢é‡æ¨¡å¼: éœ€æŸ¥è¯¢ {total_slots} ä¸ªæ§½]")
        else:
            logger.step(idx + 1, total_to_warm, f"é¢„çƒ­ {window} çª—å£ï¼Œé¢„è®¡æ‰«æ {window_estimated_logs:,} æ¡æ—¥å¿—...")

        try:
            # æŸ¥è¯¢ PostgreSQLï¼ˆåªè¯»ï¼‰
            # æ³¨æ„ï¼šè¿™é‡ŒåªæŸ¥è¯¢ Top 50 ç”¨æˆ·çš„èšåˆæ•°æ®ï¼Œä¸æ˜¯å…¨é‡æ•°æ®
            # å³ä½¿æœ‰åƒä¸‡çº§æ—¥å¿—ï¼ŒSQL ä½¿ç”¨ç´¢å¼•èšåˆï¼Œåªè¿”å› 50 æ¡ç»“æœ
            # å¯¹äº 3d/7dï¼Œä½¿ç”¨å¢é‡ç¼“å­˜æ¨¡å¼ï¼Œå¤ç”¨å·²æœ‰æ§½æ•°æ®
            loop = asyncio.get_event_loop()
            data = await loop.run_in_executor(
                None,
                lambda: service.get_leaderboards(
                    windows=[window],
                    limit=50,
                    sort_by="requests",
                    use_cache=False,
                    log_progress=True,
                ),
            )

            window_elapsed = time.time() - window_start
            window_times.append(window_elapsed)  # è®°å½•å®é™…è€—æ—¶

            if data and window in data.get("windows", {}):
                result_count = len(data["windows"][window])
                warmed.append(window)
                if is_incremental:
                    logger.success(f"{window} é¢„çƒ­å®Œæˆ [å¢é‡]", æ•°æ®=result_count, è€—æ—¶=f"{window_elapsed:.2f}s")
                else:
                    logger.success(f"{window} é¢„çƒ­å®Œæˆ", æ•°æ®=result_count, è€—æ—¶=f"{window_elapsed:.2f}s")
            else:
                failed.append(window)
                logger.warn(f"{window} æ— æ•°æ®", è€—æ—¶=f"{window_elapsed:.2f}s")

        except Exception as e:
            window_elapsed = time.time() - window_start
            window_times.append(window_elapsed)  # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•è€—æ—¶
            failed.append(window)
            logger.fail(f"{window} é¢„çƒ­å¤±è´¥", error=str(e), è€—æ—¶=f"{window_elapsed:.2f}s")

        # å»¶è¿Ÿï¼Œé¿å…æ•°æ®åº“å‹åŠ›
        if query_delay > 0 and idx < total_to_warm - 1:
            await asyncio.sleep(query_delay)

    # å®Œæˆæ±‡æ€»
    total_elapsed = time.time() - warmup_start_time
    total_cached_records = len(warmed) * 50  # æ¯ä¸ªçª—å£ 50 æ¡

    logger.divider("â•")
    if failed:
        logger.bullet(f"æˆåŠŸ: {warmed}")
        logger.bullet(f"å¤±è´¥: {failed}")
    else:
        logger.success(f"å…¨éƒ¨çª—å£é¢„çƒ­å®Œæˆ", çª—å£=warmed)

    logger.kvs({
        "å·²ç¼“å­˜æ•°æ®": f"{total_cached_records} æ¡ ({len(warmed)} çª—å£ Ã— 50 ç”¨æˆ·)",
        "æ€»è€—æ—¶": f"{total_elapsed:.1f}s",
    })

    # æ’è¡Œæ¦œçª—å£é¢„çƒ­å®Œæˆ
    steps[2]["status"] = "done" if not failed else "error"
    window_status_msg = (
        f"æ’è¡Œæ¦œé¢„çƒ­å®Œæˆï¼ˆéƒ¨åˆ†å¤±è´¥ï¼‰ï¼Œæ­£åœ¨é¢„çƒ­ Dashboard..."
        if failed
        else f"æ’è¡Œæ¦œé¢„çƒ­å®Œæˆï¼Œæ­£åœ¨é¢„çƒ­ Dashboard..."
    )
    _set_warmup_status("initializing", 50, window_status_msg, steps)

    # === é˜¶æ®µ4ï¼šé¢„çƒ­ Dashboard æ•°æ®ï¼ˆé‡è¦ï¼é¿å…é¦–æ¬¡è®¿é—®è¶…æ—¶ï¼‰===
    try:
        await _warmup_dashboard_data()
        steps[3]["status"] = "done"
    except Exception as e:
        logger.warn(f"Dashboard é¢„çƒ­å¼‚å¸¸: {e}")
        steps[3]["status"] = "error"
    _set_warmup_status("initializing", 60, "æ­£åœ¨é¢„çƒ­ç”¨æˆ·æ´»è·ƒåº¦åˆ—è¡¨...", steps)

    # === é˜¶æ®µ5ï¼šé¢„çƒ­ç”¨æˆ·æ´»è·ƒåº¦åˆ—è¡¨ï¼ˆä»…å¤§å‹ç³»ç»Ÿï¼‰===
    try:
        await _warmup_user_activity_list()
        steps[4]["status"] = "done"
    except Exception as e:
        logger.warn(f"ç”¨æˆ·æ´»è·ƒåº¦åˆ—è¡¨é¢„çƒ­å¼‚å¸¸: {e}")
        steps[4]["status"] = "error"
    _set_warmup_status("initializing", 70, "æ­£åœ¨é¢„çƒ­ IP ç›‘æ§æ•°æ®...", steps)

    # === é˜¶æ®µ5.5ï¼šé¢„çƒ­ IP ç›‘æ§æ•°æ® ===
    try:
        await _warmup_ip_monitoring_data()
        steps[5]["status"] = "done"
    except Exception as e:
        logger.warn(f"IPç›‘æ§é¢„çƒ­å¼‚å¸¸: {e}")
        steps[5]["status"] = "error"
    _set_warmup_status("initializing", 80, "æ­£åœ¨é¢„çƒ­ IP åœ°åŒºåˆ†å¸ƒ...", steps)

    # === é˜¶æ®µ6ï¼šé¢„çƒ­ IP åœ°åŒºåˆ†å¸ƒ ===
    try:
        from .ip_distribution_service import warmup_ip_distribution
        await warmup_ip_distribution()
        steps[6]["status"] = "done"
    except Exception as e:
        logger.warning(f"[IPåˆ†å¸ƒ] é¢„çƒ­å¼‚å¸¸: {e}")
        steps[6]["status"] = "error"
    _set_warmup_status("initializing", 90, "æ­£åœ¨é¢„çƒ­æ¨¡å‹çŠ¶æ€...", steps)

    # === é˜¶æ®µ7ï¼šé¢„çƒ­æ¨¡å‹çŠ¶æ€ç›‘æ§æ•°æ®ï¼ˆåŠ¨æ€è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼‰
    try:
        from .model_status_service import warmup_model_status
        await warmup_model_status()
        steps[7]["status"] = "done"
    except Exception as e:
        logger.warning(f"[æ¨¡å‹çŠ¶æ€] é¢„çƒ­å¼‚å¸¸: {e}")
        steps[7]["status"] = "error"

    # æ‰€æœ‰é¢„çƒ­å®Œæˆåè¾“å‡ºå®Œæˆæ—¥å¿—
    total_warmup_elapsed = time.time() - warmup_start_time
    has_errors = any(s["status"] == "error" for s in steps)
    final_msg = (
        f"é¢„çƒ­å®Œæˆï¼ˆéƒ¨åˆ†å¤±è´¥ï¼‰ï¼Œè€—æ—¶ {total_warmup_elapsed:.1f}s"
        if has_errors
        else f"é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {total_warmup_elapsed:.1f}s"
    )
    _set_warmup_status("ready", 100, final_msg, steps)
    logger.banner("âœ… ç¼“å­˜é¢„çƒ­å®Œæˆ")
    logger.kvs({
        "æ€»è€—æ—¶": f"{total_warmup_elapsed:.1f}s",
    })

    # é¢„çƒ­å®Œæˆåå¯åŠ¨åå°ä»»åŠ¡
    asyncio.create_task(background_log_sync())
    asyncio.create_task(background_ai_auto_ban_scan())

    # è¿›å…¥å®šæ—¶åˆ·æ–°å¾ªç¯
    await _background_refresh_loop(cache)


async def _background_refresh_loop(cache):
    """
    åå°å®šæ—¶åˆ·æ–°ç¼“å­˜

    åˆ·æ–°å†…å®¹ï¼š
    1. æ’è¡Œæ¦œæ•°æ®ï¼ˆæ‰€æœ‰æ—¶é—´çª—å£ï¼‰
    2. ä»ªè¡¨ç›˜æ ¸å¿ƒæ•°æ®ï¼ˆé¿å…ç”¨æˆ·è®¿é—®æ—¶è§¦å‘æ…¢æŸ¥è¯¢ï¼‰
    3. æ¨¡å‹çŠ¶æ€æ•°æ®ï¼ˆæ¨¡å‹åˆ—è¡¨å’ŒçŠ¶æ€ç¼“å­˜ï¼‰

    é’ˆå¯¹å¤§å‹ç³»ç»Ÿä¼˜åŒ–ï¼š
    - æ ¹æ®ç³»ç»Ÿè§„æ¨¡è°ƒæ•´åˆ·æ–°é—´éš”
    - åˆ†æ‰¹åˆ·æ–°é¿å…ç¬é—´é«˜è´Ÿè½½
    - ä»ªè¡¨ç›˜æ•°æ®æ¯ 3 ä¸ªå‘¨æœŸåˆ·æ–°ä¸€æ¬¡
    - æ¨¡å‹çŠ¶æ€æ•°æ®æ¯ 6 ä¸ªå‘¨æœŸåˆ·æ–°ä¸€æ¬¡ï¼ˆçº¦ 30 åˆ†é’Ÿï¼‰
    """
    from .system_scale_service import get_detected_settings
    from .risk_monitoring_service import get_risk_monitoring_service
    from .cached_dashboard import get_cached_dashboard_service
    from .model_status_service import get_model_status_service

    windows = ["1h", "3h", "6h", "12h", "24h", "3d", "7d"]
    dashboard_refresh_counter = 0  # ä»ªè¡¨ç›˜åˆ·æ–°è®¡æ•°å™¨
    model_status_refresh_counter = 0  # æ¨¡å‹çŠ¶æ€åˆ·æ–°è®¡æ•°å™¨

    while True:
        try:
            settings = get_detected_settings()
            interval = settings.leaderboard_cache_ttl

            logger.debug(f"[å®šæ—¶åˆ·æ–°] ä¸‹æ¬¡åˆ·æ–°åœ¨ {interval}s å")
            await asyncio.sleep(interval)

            refresh_start = time.time()

            # === åˆ·æ–°æ’è¡Œæ¦œæ•°æ® ===
            service = get_risk_monitoring_service()
            for window in windows:
                try:
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(
                        None,
                        lambda w=window: service.get_leaderboards(
                            windows=[w],
                            limit=50,
                            use_cache=False,
                        ),
                    )
                    # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…ç¬é—´é«˜è´Ÿè½½
                    await asyncio.sleep(0.5)
                except Exception:
                    pass

            # === åˆ·æ–°ä»ªè¡¨ç›˜æ•°æ®ï¼ˆæ¯ 3 ä¸ªå‘¨æœŸåˆ·æ–°ä¸€æ¬¡ï¼‰===
            dashboard_refresh_counter += 1
            if dashboard_refresh_counter >= 3:
                dashboard_refresh_counter = 0
                try:
                    dashboard_service = get_cached_dashboard_service()
                    loop = asyncio.get_event_loop()

                    # åˆ·æ–°æ ¸å¿ƒä»ªè¡¨ç›˜æ•°æ®
                    await loop.run_in_executor(
                        None,
                        lambda: dashboard_service.get_system_overview(period="7d", use_cache=False)
                    )
                    await asyncio.sleep(0.5)

                    await loop.run_in_executor(
                        None,
                        lambda: dashboard_service.get_usage_statistics(period="7d", use_cache=False)
                    )
                    await asyncio.sleep(0.5)

                    await loop.run_in_executor(
                        None,
                        lambda: dashboard_service.get_daily_trends(days=7, use_cache=False)
                    )
                    await asyncio.sleep(0.5)

                    await loop.run_in_executor(
                        None,
                        lambda: dashboard_service.get_top_users(period="7d", limit=10, use_cache=False)
                    )

                    logger.debug("[å®šæ—¶åˆ·æ–°] ä»ªè¡¨ç›˜æ•°æ®å·²åˆ·æ–°")
                except Exception as e:
                    logger.warning(f"[å®šæ—¶åˆ·æ–°] ä»ªè¡¨ç›˜åˆ·æ–°å¤±è´¥: {e}")

            # === åˆ·æ–°æ¨¡å‹çŠ¶æ€æ•°æ®ï¼ˆæ¯ 6 ä¸ªå‘¨æœŸåˆ·æ–°ä¸€æ¬¡ï¼Œçº¦ 30 åˆ†é’Ÿï¼‰===
            model_status_refresh_counter += 1
            if model_status_refresh_counter >= 6:
                model_status_refresh_counter = 0
                try:
                    model_service = get_model_status_service()
                    loop = asyncio.get_event_loop()

                    # åˆ·æ–°æ¨¡å‹åˆ—è¡¨ï¼ˆå« 24h è¯·æ±‚ç»Ÿè®¡ï¼‰
                    await loop.run_in_executor(
                        None,
                        lambda: model_service.get_available_models_with_stats(use_cache=False)
                    )
                    logger.debug("[å®šæ—¶åˆ·æ–°] æ¨¡å‹çŠ¶æ€æ•°æ®å·²åˆ·æ–°")
                except Exception as e:
                    logger.warning(f"[å®šæ—¶åˆ·æ–°] æ¨¡å‹çŠ¶æ€åˆ·æ–°å¤±è´¥: {e}")

            refresh_elapsed = time.time() - refresh_start
            logger.debug(f"[å®šæ—¶åˆ·æ–°] å®Œæˆï¼Œè€—æ—¶ {refresh_elapsed:.1f}s")

        except asyncio.CancelledError:
            logger.system("ç¼“å­˜åˆ·æ–°ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.warning(f"[å®šæ—¶åˆ·æ–°] å¤±è´¥: {e}")
            await asyncio.sleep(60)


# é¢„çƒ­çŠ¶æ€å­˜å‚¨
_warmup_state = {
    "status": "pending",  # pending, initializing, ready
    "progress": 0,
    "message": "ç­‰å¾…å¯åŠ¨...",
    "steps": [],
    "started_at": None,
    "completed_at": None,
}
_warmup_lock = threading.Lock()


def _set_warmup_status(status: str, progress: int, message: str, steps: list = None):
    """æ›´æ–°é¢„çƒ­çŠ¶æ€"""
    global _warmup_state
    with _warmup_lock:
        _warmup_state["status"] = status
        _warmup_state["progress"] = progress
        _warmup_state["message"] = message
        if steps is not None:
            _warmup_state["steps"] = steps
        if status == "initializing" and _warmup_state["started_at"] is None:
            _warmup_state["started_at"] = time.time()
        if status == "ready":
            _warmup_state["completed_at"] = time.time()


def get_warmup_status() -> dict:
    """è·å–é¢„çƒ­çŠ¶æ€ï¼ˆä¾› API è°ƒç”¨ï¼‰"""
    with _warmup_lock:
        return _warmup_state.copy()


import threading


# æ ¹æ®ç³»ç»Ÿè§„æ¨¡å®šä¹‰é¢„çƒ­ç­–ç•¥
# æ‰€æœ‰è§„æ¨¡éƒ½é¢„çƒ­å…¨éƒ¨çª—å£ï¼Œåªæ˜¯å»¶è¿Ÿæ—¶é—´ä¸åŒ
WARMUP_STRATEGY = {
    # scale: {
    #   windows: é¢„çƒ­çš„æ—¶é—´çª—å£ï¼ˆå…¨éƒ¨çª—å£ï¼‰
    #   query_delay: æ¯ä¸ªæŸ¥è¯¢ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œè§„æ¨¡è¶Šå¤§å»¶è¿Ÿè¶Šé•¿
    #   ip_window: IP ç›‘æ§ä½¿ç”¨çš„æ—¶é—´çª—å£
    #   limit: æ’è¡Œæ¦œæŸ¥è¯¢æ•°é‡é™åˆ¶
    # }
    "small": {
        "windows": ["1h", "3h", "6h", "12h", "24h", "3d", "7d"],
        "query_delay": 0.5,
        "ip_window": "24h",
        "limit": 10,
    },
    "medium": {
        "windows": ["1h", "3h", "6h", "12h", "24h", "3d", "7d"],
        "query_delay": 1.5,
        "ip_window": "24h",
        "limit": 10,
    },
    "large": {
        "windows": ["1h", "3h", "6h", "12h", "24h", "3d", "7d"],
        "query_delay": 3.0,
        "ip_window": "24h",
        "limit": 10,
    },
    "xlarge": {
        "windows": ["1h", "3h", "6h", "12h", "24h", "3d", "7d"],
        "query_delay": 5.0,  # è¶…å¤§è§„æ¨¡ç³»ç»Ÿï¼Œå»¶è¿Ÿæ›´é•¿
        "ip_window": "24h",
        "limit": 10,
    },
}


async def _do_complete_warmup(scale):
    """
    æ‰§è¡Œå®Œæ•´çš„æ¸è¿›å¼ç¼“å­˜é¢„çƒ­

    Args:
        scale: SystemScale æšä¸¾å€¼

    é¢„çƒ­é¡ºåºï¼ˆå…¨éƒ¨å®Œæˆåæ‰æ ‡è®°ä¸ºå°±ç»ªï¼‰ï¼š
    1. æ’è¡Œæ¦œæ•°æ®ï¼šé€ä¸ªçª—å£é¢„çƒ­ï¼ˆ1h â†’ 3h â†’ 6h â†’ 12h â†’ 24h â†’ 3d â†’ 7dï¼‰
    2. IP ç›‘æ§æ•°æ®ï¼šå…±äº«IPã€å¤šIPä»¤ç‰Œã€å¤šIPç”¨æˆ·
    3. ç”¨æˆ·ç»Ÿè®¡æ•°æ®
    """
    import asyncio

    strategy = WARMUP_STRATEGY.get(scale.value, WARMUP_STRATEGY["medium"])
    windows = strategy["windows"]
    query_delay = strategy["query_delay"]
    ip_window = strategy["ip_window"]
    limit = strategy["limit"]

    logger.system(f"å¼€å§‹å®Œæ•´é¢„çƒ­: çª—å£ {windows}, æŸ¥è¯¢å»¶è¿Ÿ {query_delay}s")

    try:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None,
            lambda: _warmup_complete_sync(
                windows=windows,
                query_delay=query_delay,
                ip_window=ip_window,
                limit=limit,
            )
        )
    except Exception as e:
        logger.warning(f"å®Œæ•´é¢„çƒ­å¼‚å¸¸: {e}", category="ç¼“å­˜")
        _set_warmup_status("ready", 100, "é¢„çƒ­å®Œæˆï¼ˆéƒ¨åˆ†å¤±è´¥ï¼‰")


def _warmup_complete_sync(
    windows: list,
    query_delay: float,
    ip_window: str,
    limit: int,
):
    """
    åŒæ­¥æ‰§è¡Œå®Œæ•´çš„æ¸è¿›å¼ç¼“å­˜é¢„çƒ­ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰

    é‡‡ç”¨æ¸©å’Œç­–ç•¥ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½é¢„çƒ­å®Œæˆï¼š
    1. é€ä¸ªçª—å£é¢„çƒ­æ’è¡Œæ¦œï¼Œæ¯ä¸ªæŸ¥è¯¢ä¹‹é—´æœ‰å»¶è¿Ÿ
    2. é€ä¸ªæŸ¥è¯¢é¢„çƒ­ IP ç›‘æ§
    3. é¢„çƒ­ç”¨æˆ·ç»Ÿè®¡

    å®¹é”™æœºåˆ¶ï¼š
    - å•ä¸ªæŸ¥è¯¢è¶…æ—¶æ§åˆ¶ï¼ˆé»˜è®¤60ç§’ï¼‰
    - å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š2æ¬¡ï¼‰
    - éƒ¨åˆ†å¤±è´¥ä¸é˜»å¡å…¶ä»–æ­¥éª¤
    - è¯¦ç»†çš„é”™è¯¯è¿½è¸ª
    """
    from .risk_monitoring_service import get_risk_monitoring_service
    from .ip_monitoring_service import get_ip_monitoring_service, WINDOW_SECONDS
    from .user_management_service import get_user_management_service
    from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError

    start_time = time.time()
    warmed = []
    failed = []
    steps = []
    total_windows = len(windows)
    step_times = []
    errors_detail = []  # è¯¦ç»†é”™è¯¯è®°å½•

    # å®¹é”™é…ç½®
    QUERY_TIMEOUT = 120  # å•ä¸ªæŸ¥è¯¢è¶…æ—¶ï¼ˆç§’ï¼‰- å¤§æ•°æ®é‡éœ€è¦æ›´é•¿æ—¶é—´
    MAX_RETRIES = 2      # æœ€å¤§é‡è¯•æ¬¡æ•°
    RETRY_DELAY = 5      # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰

    # è®¡ç®—æ€»æ­¥éª¤æ•°
    total_steps = total_windows + 3 + 1
    current_step = 0

    def update_progress(message: str, step_name: str = None, step_status: str = None):
        nonlocal current_step
        current_step += 1
        progress = 10 + int((current_step / total_steps) * 85)
        if step_name and step_status:
            steps.append({"name": step_name, "status": step_status})
        _set_warmup_status("initializing", min(progress, 95), message, steps)

    def execute_with_timeout_and_retry(func, name: str, timeout: int = QUERY_TIMEOUT) -> tuple:
        """
        å¸¦è¶…æ—¶å’Œé‡è¯•çš„æŸ¥è¯¢æ‰§è¡Œå™¨

        Returns:
            (success: bool, elapsed: float, error: str or None)
        """
        last_error = None

        for attempt in range(MAX_RETRIES + 1):
            query_start = time.time()
            try:
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(func)
                    future.result(timeout=timeout)

                elapsed = time.time() - query_start
                if attempt > 0:
                    logger.system(f"[é¢„çƒ­] {name}: é‡è¯•æˆåŠŸ (å°è¯• {attempt + 1})")
                return True, elapsed, None

            except FuturesTimeoutError:
                elapsed = time.time() - query_start
                last_error = f"è¶…æ—¶ ({timeout}s)"
                logger.warning(f"[é¢„çƒ­] {name}: è¶…æ—¶ ({elapsed:.1f}s > {timeout}s)", category="ç¼“å­˜")

            except Exception as e:
                elapsed = time.time() - query_start
                last_error = str(e)
                logger.warning(f"[é¢„çƒ­] {name}: å¤±è´¥ - {e}", category="ç¼“å­˜")

            # é‡è¯•å‰ç­‰å¾…
            if attempt < MAX_RETRIES:
                retry_wait = RETRY_DELAY * (attempt + 1)  # é€’å¢ç­‰å¾…
                logger.system(f"[é¢„çƒ­] {name}: ç­‰å¾… {retry_wait}s åé‡è¯• ({attempt + 2}/{MAX_RETRIES + 1})")
                time.sleep(retry_wait)

        return False, time.time() - query_start, last_error

    # === Step 1: é€ä¸ªé¢„çƒ­é£æ§æ’è¡Œæ¦œçª—å£ ===
    logger.system(f"[é¢„çƒ­] æ’è¡Œæ¦œ: å…± {total_windows} ä¸ªçª—å£, è¶…æ—¶={QUERY_TIMEOUT}s, é‡è¯•={MAX_RETRIES}æ¬¡")
    _set_warmup_status("initializing", 10, f"æ­£åœ¨åŠ è½½æ’è¡Œæ¦œæ•°æ® (0/{total_windows})...", steps)

    leaderboard_start = time.time()
    leaderboard_success = 0
    leaderboard_failed = 0

    try:
        risk_service = get_risk_monitoring_service()

        for idx, window in enumerate(windows):
            update_progress(f"æ­£åœ¨åŠ è½½æ’è¡Œæ¦œ: {window} ({idx + 1}/{total_windows})...")

            def query_leaderboard():
                risk_service.get_leaderboards(
                    windows=[window],
                    limit=limit,
                    sort_by="requests",
                    use_cache=False,
                )

            success, elapsed, error = execute_with_timeout_and_retry(
                query_leaderboard,
                f"æ’è¡Œæ¦œ {window}",
                timeout=QUERY_TIMEOUT
            )

            if success:
                leaderboard_success += 1
                logger.system(f"[é¢„çƒ­] æ’è¡Œæ¦œ {window}: {elapsed:.2f}s âœ“")
            else:
                leaderboard_failed += 1
                errors_detail.append(f"æ’è¡Œæ¦œ {window}: {error}")
                logger.warning(f"[é¢„çƒ­] æ’è¡Œæ¦œ {window}: å¤±è´¥ âœ— ({error})", category="ç¼“å­˜")

            # å»¶è¿Ÿ
            if query_delay > 0:
                time.sleep(query_delay)

        leaderboard_elapsed = time.time() - leaderboard_start
        step_times.append(f"æ’è¡Œæ¦œ={leaderboard_elapsed:.1f}s({leaderboard_success}/{total_windows})")

        if leaderboard_failed == 0:
            warmed.append(f"æ’è¡Œæ¦œ({total_windows}ä¸ªçª—å£)")
            steps.append({"name": "æ’è¡Œæ¦œ", "status": "done"})
        elif leaderboard_success > 0:
            warmed.append(f"æ’è¡Œæ¦œ({leaderboard_success}/{total_windows})")
            failed.append(f"æ’è¡Œæ¦œ({leaderboard_failed}å¤±è´¥)")
            steps.append({"name": "æ’è¡Œæ¦œ", "status": "partial"})
        else:
            failed.append("æ’è¡Œæ¦œ(å…¨éƒ¨å¤±è´¥)")
            steps.append({"name": "æ’è¡Œæ¦œ", "status": "error"})

        logger.system(f"[é¢„çƒ­] æ’è¡Œæ¦œå®Œæˆ: {leaderboard_success}/{total_windows} æˆåŠŸ, è€—æ—¶ {leaderboard_elapsed:.1f}s")

    except Exception as e:
        logger.error(f"[é¢„çƒ­] æ’è¡Œæ¦œæœåŠ¡å¼‚å¸¸: {e}", category="ç¼“å­˜")
        steps.append({"name": "æ’è¡Œæ¦œ", "status": "error", "error": str(e)})
        failed.append("æ’è¡Œæ¦œ(æœåŠ¡å¼‚å¸¸)")
        errors_detail.append(f"æ’è¡Œæ¦œæœåŠ¡: {e}")

    # === Step 2: é¢„çƒ­ IP ç›‘æ§æ•°æ®ï¼ˆå¤šçª—å£ + å¤§ limitï¼‰===
    # é¢„çƒ­å¤šä¸ªæ—¶é—´çª—å£ï¼ŒåŒ¹é…å‰ç«¯è¯·æ±‚çš„ limit=200
    IP_WARMUP_LIMIT = 200  # å‰ç«¯è¯·æ±‚çš„æœ€å¤§ limit
    IP_WARMUP_WINDOWS = ["1h", "24h", "7d"]  # é¢„çƒ­çš„æ—¶é—´çª—å£

    logger.system(f"[é¢„çƒ­] IPç›‘æ§: çª—å£={IP_WARMUP_WINDOWS}, limit={IP_WARMUP_LIMIT}")
    ip_start = time.time()
    ip_success = 0
    ip_failed = 0
    ip_total = len(IP_WARMUP_WINDOWS) * 3  # 3ç§æŸ¥è¯¢ Ã— Nä¸ªçª—å£

    try:
        ip_service = get_ip_monitoring_service()

        for window_key in IP_WARMUP_WINDOWS:
            window_seconds = WINDOW_SECONDS.get(window_key, 86400)

            ip_queries = [
                (f"å…±äº«IP({window_key})", lambda ws=window_seconds: ip_service.get_shared_ips(
                    window_seconds=ws, min_tokens=2, limit=IP_WARMUP_LIMIT, use_cache=False
                )),
                (f"å¤šIPä»¤ç‰Œ({window_key})", lambda ws=window_seconds: ip_service.get_multi_ip_tokens(
                    window_seconds=ws, min_ips=2, limit=IP_WARMUP_LIMIT, use_cache=False
                )),
                (f"å¤šIPç”¨æˆ·({window_key})", lambda ws=window_seconds: ip_service.get_multi_ip_users(
                    window_seconds=ws, min_ips=3, limit=IP_WARMUP_LIMIT, use_cache=False
                )),
            ]

            for query_name, query_func in ip_queries:
                update_progress(f"æ­£åœ¨åŠ è½½{query_name}æ•°æ®...")

                success, elapsed, error = execute_with_timeout_and_retry(
                    query_func,
                    query_name,
                    timeout=QUERY_TIMEOUT
                )

                if success:
                    ip_success += 1
                    logger.system(f"[é¢„çƒ­] {query_name}: {elapsed:.2f}s âœ“")
                else:
                    ip_failed += 1
                    errors_detail.append(f"{query_name}: {error}")
                    logger.warning(f"[é¢„çƒ­] {query_name}: å¤±è´¥ âœ— ({error})", category="ç¼“å­˜")

                if query_delay > 0:
                    time.sleep(query_delay)

        ip_elapsed = time.time() - ip_start
        step_times.append(f"IPç›‘æ§={ip_elapsed:.1f}s({ip_success}/{ip_total})")

        if ip_failed == 0:
            warmed.append(f"IPç›‘æ§({len(IP_WARMUP_WINDOWS)}çª—å£)")
            steps.append({"name": "IPç›‘æ§", "status": "done"})
        elif ip_success > 0:
            warmed.append(f"IPç›‘æ§({ip_success}/{ip_total})")
            failed.append(f"IPç›‘æ§({ip_failed}å¤±è´¥)")
            steps.append({"name": "IPç›‘æ§", "status": "partial"})
        else:
            failed.append("IPç›‘æ§(å…¨éƒ¨å¤±è´¥)")
            steps.append({"name": "IPç›‘æ§", "status": "error"})

        # é¢„çƒ­ IP Statsï¼ˆIPè®°å½•çŠ¶æ€ç»Ÿè®¡ï¼‰
        update_progress("æ­£åœ¨åŠ è½½IPè®°å½•çŠ¶æ€...")
        success, elapsed, error = execute_with_timeout_and_retry(
            lambda: ip_service.get_ip_recording_stats(use_cache=False),
            "IP Stats",
            timeout=QUERY_TIMEOUT
        )
        if success:
            logger.system(f"[é¢„çƒ­] IP Stats: {elapsed:.2f}s âœ“")
        else:
            logger.warning(f"[é¢„çƒ­] IP Stats: å¤±è´¥ âœ— ({error})", category="ç¼“å­˜")

        logger.system(f"[é¢„çƒ­] IPç›‘æ§å®Œæˆ: {ip_success}/{ip_total} æˆåŠŸ, è€—æ—¶ {ip_elapsed:.1f}s")

    except Exception as e:
        logger.error(f"[é¢„çƒ­] IPç›‘æ§æœåŠ¡å¼‚å¸¸: {e}", category="ç¼“å­˜")
        steps.append({"name": "IPç›‘æ§", "status": "error", "error": str(e)})
        failed.append("IPç›‘æ§(æœåŠ¡å¼‚å¸¸)")
        errors_detail.append(f"IPç›‘æ§æœåŠ¡: {e}")

    # === Step 3: é¢„çƒ­ç”¨æˆ·ç»Ÿè®¡ ===
    logger.system("[é¢„çƒ­] ç”¨æˆ·ç»Ÿè®¡")
    update_progress("æ­£åœ¨åŠ è½½ç”¨æˆ·ç»Ÿè®¡æ•°æ®...")
    stats_start = time.time()

    try:
        user_service = get_user_management_service()

        def query_stats():
            user_service.get_activity_stats()

        success, elapsed, error = execute_with_timeout_and_retry(
            query_stats,
            "ç”¨æˆ·ç»Ÿè®¡",
            timeout=QUERY_TIMEOUT
        )

        if success:
            step_times.append(f"ç”¨æˆ·ç»Ÿè®¡={elapsed:.1f}s")
            warmed.append("ç”¨æˆ·ç»Ÿè®¡")
            steps.append({"name": "ç”¨æˆ·ç»Ÿè®¡", "status": "done"})
            logger.system(f"[é¢„çƒ­] ç”¨æˆ·ç»Ÿè®¡: {elapsed:.2f}s âœ“")
        else:
            failed.append("ç”¨æˆ·ç»Ÿè®¡")
            errors_detail.append(f"ç”¨æˆ·ç»Ÿè®¡: {error}")
            steps.append({"name": "ç”¨æˆ·ç»Ÿè®¡", "status": "error", "error": error})
            logger.warning(f"[é¢„çƒ­] ç”¨æˆ·ç»Ÿè®¡: å¤±è´¥ âœ— ({error})", category="ç¼“å­˜")

    except Exception as e:
        logger.error(f"[é¢„çƒ­] ç”¨æˆ·ç»Ÿè®¡æœåŠ¡å¼‚å¸¸: {e}", category="ç¼“å­˜")
        steps.append({"name": "ç”¨æˆ·ç»Ÿè®¡", "status": "error", "error": str(e)})
        failed.append("ç”¨æˆ·ç»Ÿè®¡(æœåŠ¡å¼‚å¸¸)")
        errors_detail.append(f"ç”¨æˆ·ç»Ÿè®¡æœåŠ¡: {e}")

    elapsed = time.time() - start_time

    # ç¡®å®šæœ€ç»ˆçŠ¶æ€
    if failed:
        status_msg = f"é¢„çƒ­å®Œæˆï¼ˆéƒ¨åˆ†å¤±è´¥ï¼‰ï¼Œè€—æ—¶ {elapsed:.1f}s"
    else:
        status_msg = f"é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}s"

    _set_warmup_status("ready", 100, status_msg, steps)

    # è¾“å‡ºé¢„çƒ­æ‘˜è¦
    logger.system("=" * 50)
    logger.system("[é¢„çƒ­æ‘˜è¦]")
    logger.system(f"  æˆåŠŸ: {', '.join(warmed) if warmed else 'æ— '}")
    if failed:
        logger.system(f"  å¤±è´¥: {', '.join(failed)}")
    logger.system(f"  å„æ­¥è€—æ—¶: {', '.join(step_times)}")
    logger.system(f"  æ€»è€—æ—¶: {elapsed:.1f}s")

    if errors_detail:
        logger.system("-" * 30)
        logger.system("[é”™è¯¯è¯¦æƒ…]")
        for err in errors_detail:
            logger.system(f"  - {err}")

    logger.system("=" * 50)


async def _do_cache_warmup(is_initial: bool = False):
    """æ‰§è¡Œç¼“å­˜é¢„çƒ­"""
    import asyncio
    
    try:
        loop = asyncio.get_event_loop()
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ“ä½œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        await loop.run_in_executor(None, lambda: _warmup_sync(is_initial))
        
    except Exception as e:
        logger.warning(f"ç¼“å­˜é¢„çƒ­å¼‚å¸¸: {e}", category="ç¼“å­˜")
        if is_initial:
            _set_warmup_status("ready", 100, "é¢„çƒ­å®Œæˆï¼ˆéƒ¨åˆ†å¤±è´¥ï¼‰")


def _warmup_sync(is_initial: bool = False):
    """
    åŒæ­¥æ‰§è¡Œç¼“å­˜é¢„çƒ­ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰

    ç”¨äºå®šæœŸåˆ·æ–°ç¼“å­˜ï¼Œé‡‡ç”¨æ¸©å’Œç­–ç•¥ï¼š
    - é€ä¸ªçª—å£é¢„çƒ­ï¼Œæ¯ä¸ªæŸ¥è¯¢ä¹‹é—´æœ‰å»¶è¿Ÿ
    - æ ¹æ®ç³»ç»Ÿè§„æ¨¡è°ƒæ•´å‚æ•°
    - å¸¦è¶…æ—¶å’Œé‡è¯•çš„å®¹é”™æœºåˆ¶
    """
    from .risk_monitoring_service import get_risk_monitoring_service
    from .ip_monitoring_service import get_ip_monitoring_service, WINDOW_SECONDS
    from .user_management_service import get_user_management_service
    from .system_scale_service import get_detected_settings
    from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError

    start_time = time.time()
    warmed = []
    failed = []

    # å®šæ—¶åˆ·æ–°çš„å®¹é”™é…ç½®ï¼ˆæ¯”åˆå§‹é¢„çƒ­æ›´å®½æ¾ï¼‰
    REFRESH_TIMEOUT = 60  # å•ä¸ªæŸ¥è¯¢è¶…æ—¶ï¼ˆç§’ï¼‰
    REFRESH_RETRIES = 1   # æœ€å¤§é‡è¯•æ¬¡æ•°

    def execute_with_timeout(func, name: str) -> bool:
        """å¸¦è¶…æ—¶å’Œé‡è¯•çš„æŸ¥è¯¢æ‰§è¡Œå™¨ï¼ˆå®šæ—¶åˆ·æ–°ç‰ˆæœ¬ï¼‰"""
        for attempt in range(REFRESH_RETRIES + 1):
            try:
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(func)
                    future.result(timeout=REFRESH_TIMEOUT)
                return True
            except FuturesTimeoutError:
                logger.warning(f"[åˆ·æ–°] {name}: è¶…æ—¶ ({REFRESH_TIMEOUT}s)", category="ç¼“å­˜")
            except Exception as e:
                logger.warning(f"[åˆ·æ–°] {name}: å¤±è´¥ - {e}", category="ç¼“å­˜")

            if attempt < REFRESH_RETRIES:
                time.sleep(2)  # çŸ­æš‚ç­‰å¾…åé‡è¯•

        return False

    # è·å–å½“å‰ç³»ç»Ÿè§„æ¨¡è®¾ç½®
    settings = get_detected_settings()
    scale = settings.scale.value

    # æ ¹æ®ç³»ç»Ÿè§„æ¨¡ç¡®å®šç­–ç•¥
    strategy = WARMUP_STRATEGY.get(scale, WARMUP_STRATEGY["medium"])
    query_delay = strategy["query_delay"]
    all_windows = strategy["windows"]
    ip_window = strategy["ip_window"]

    # Step 1: é€ä¸ªé¢„çƒ­é£æ§æ’è¡Œæ¦œçª—å£ï¼ˆæ¸©å’Œæ–¹å¼ï¼‰
    leaderboard_success = 0
    leaderboard_failed = 0

    try:
        risk_service = get_risk_monitoring_service()

        for idx, window in enumerate(all_windows):
            def query_leaderboard():
                risk_service.get_leaderboards(
                    windows=[window],
                    limit=10,
                    sort_by="requests",
                    use_cache=False,
                )

            if execute_with_timeout(query_leaderboard, f"æ’è¡Œæ¦œ {window}"):
                leaderboard_success += 1
            else:
                leaderboard_failed += 1

            # å»¶è¿Ÿï¼Œç»™æ•°æ®åº“å–˜æ¯çš„æœºä¼š
            if query_delay > 0 and idx < len(all_windows) - 1:
                time.sleep(query_delay)

        if leaderboard_failed == 0:
            warmed.append("æ’è¡Œæ¦œ")
        elif leaderboard_success > 0:
            warmed.append(f"æ’è¡Œæ¦œ({leaderboard_success}/{len(all_windows)})")
            failed.append(f"æ’è¡Œæ¦œ({leaderboard_failed}å¤±è´¥)")
        else:
            failed.append("æ’è¡Œæ¦œ")
    except Exception as e:
        logger.warning(f"æ’è¡Œæ¦œæœåŠ¡å¼‚å¸¸: {e}", category="ç¼“å­˜")
        failed.append("æ’è¡Œæ¦œ(æœåŠ¡å¼‚å¸¸)")

    # å»¶è¿Ÿåç»§ç»­
    if query_delay > 0:
        time.sleep(query_delay)

    # Step 2: é¢„çƒ­ IP ç›‘æ§æ•°æ®ï¼ˆå¤šçª—å£ + å¤§ limitï¼‰
    IP_REFRESH_LIMIT = 200  # åŒ¹é…å‰ç«¯è¯·æ±‚çš„ limit
    IP_REFRESH_WINDOWS = ["1h", "24h", "7d"]  # åˆ·æ–°çš„æ—¶é—´çª—å£

    ip_success = 0
    ip_failed = 0
    ip_total = len(IP_REFRESH_WINDOWS) * 3

    try:
        ip_service = get_ip_monitoring_service()

        for window_key in IP_REFRESH_WINDOWS:
            window_seconds = WINDOW_SECONDS.get(window_key, 86400)

            # å…±äº« IP
            if execute_with_timeout(
                lambda ws=window_seconds: ip_service.get_shared_ips(
                    window_seconds=ws,
                    min_tokens=2,
                    limit=IP_REFRESH_LIMIT,
                    use_cache=False
                ),
                f"å…±äº«IP({window_key})"
            ):
                ip_success += 1
            else:
                ip_failed += 1

            if query_delay > 0:
                time.sleep(query_delay)

            # å¤š IP ä»¤ç‰Œ
            if execute_with_timeout(
                lambda ws=window_seconds: ip_service.get_multi_ip_tokens(
                    window_seconds=ws,
                    min_ips=2,
                    limit=IP_REFRESH_LIMIT,
                    use_cache=False
                ),
                f"å¤šIPä»¤ç‰Œ({window_key})"
            ):
                ip_success += 1
            else:
                ip_failed += 1

            if query_delay > 0:
                time.sleep(query_delay)

            # å¤š IP ç”¨æˆ·
            if execute_with_timeout(
                lambda ws=window_seconds: ip_service.get_multi_ip_users(
                    window_seconds=ws,
                    min_ips=3,
                    limit=IP_REFRESH_LIMIT,
                    use_cache=False
                ),
                f"å¤šIPç”¨æˆ·({window_key})"
            ):
                ip_success += 1
            else:
                ip_failed += 1

            if query_delay > 0:
                time.sleep(query_delay)

        if ip_failed == 0:
            warmed.append(f"IPç›‘æ§({len(IP_REFRESH_WINDOWS)}çª—å£)")
        elif ip_success > 0:
            warmed.append(f"IPç›‘æ§({ip_success}/{ip_total})")
            failed.append(f"IPç›‘æ§({ip_failed}å¤±è´¥)")
        else:
            failed.append("IPç›‘æ§")

        # åˆ·æ–° IP Statsï¼ˆIPè®°å½•çŠ¶æ€ç»Ÿè®¡ï¼‰
        if execute_with_timeout(
            lambda: ip_service.get_ip_recording_stats(use_cache=False),
            "IP Stats"
        ):
            pass  # IP Stats ä¸å•ç‹¬è®¡å…¥ warmedï¼ŒåŒ…å«åœ¨ IPç›‘æ§ ä¸­
        # IP Stats å¤±è´¥ä¸å•ç‹¬æŠ¥å‘Š

    except Exception as e:
        logger.warning(f"IPç›‘æ§æœåŠ¡å¼‚å¸¸: {e}", category="ç¼“å­˜")
        failed.append("IPç›‘æ§(æœåŠ¡å¼‚å¸¸)")

    # å»¶è¿Ÿåç»§ç»­
    if query_delay > 0:
        time.sleep(query_delay)

    # Step 3: é¢„çƒ­ç”¨æˆ·ç»Ÿè®¡
    try:
        user_service = get_user_management_service()
        if execute_with_timeout(
            lambda: user_service.get_activity_stats(),
            "ç”¨æˆ·ç»Ÿè®¡"
        ):
            warmed.append("ç”¨æˆ·ç»Ÿè®¡")
        else:
            failed.append("ç”¨æˆ·ç»Ÿè®¡")
    except Exception as e:
        logger.warning(f"ç”¨æˆ·ç»Ÿè®¡æœåŠ¡å¼‚å¸¸: {e}", category="ç¼“å­˜")
        failed.append("ç”¨æˆ·ç»Ÿè®¡(æœåŠ¡å¼‚å¸¸)")

    elapsed = time.time() - start_time

    # è¾“å‡ºåˆ·æ–°ç»“æœ
    if warmed and not failed:
        logger.system(f"å®šæ—¶ç¼“å­˜åˆ·æ–°å®Œæˆ: {', '.join(warmed)} | è€—æ—¶ {elapsed:.2f}s")
    elif warmed:
        logger.system(f"å®šæ—¶ç¼“å­˜åˆ·æ–°éƒ¨åˆ†å®Œæˆ: æˆåŠŸ=[{', '.join(warmed)}] å¤±è´¥=[{', '.join(failed)}] | è€—æ—¶ {elapsed:.2f}s")
    elif failed:
        logger.warning(f"å®šæ—¶ç¼“å­˜åˆ·æ–°å¤±è´¥: {', '.join(failed)} | è€—æ—¶ {elapsed:.2f}s", category="ç¼“å­˜")


async def background_ai_auto_ban_scan():
    """åå°å®šæ—¶æ‰§è¡Œ AI è‡ªåŠ¨å°ç¦æ‰«æ"""
    from .ai_auto_ban_service import get_ai_auto_ban_service

    # é¢„çƒ­å®Œæˆåç«‹å³å¯åŠ¨
    logger.success("AI è‡ªåŠ¨å°ç¦åå°ä»»åŠ¡å·²å¯åŠ¨", category="ä»»åŠ¡")

    while True:
        try:
            service = get_ai_auto_ban_service()

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å®šæ—¶æ‰«æ
            scan_interval = service.get_scan_interval()
            if scan_interval <= 0:
                # å®šæ—¶æ‰«æå·²å…³é—­ï¼Œç­‰å¾… 1 åˆ†é’Ÿåå†æ£€æŸ¥é…ç½®
                await asyncio.sleep(60)
                continue

            # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
            if not service.is_enabled():
                await asyncio.sleep(60)
                continue

            # å…ˆç­‰å¾…é…ç½®çš„æ‰«æé—´éš”ï¼Œå†æ‰§è¡Œæ‰«æ
            logger.system(f"AI è‡ªåŠ¨å°ç¦: ç­‰å¾… {scan_interval} åˆ†é’Ÿåæ‰§è¡Œå®šæ—¶æ‰«æ")
            await asyncio.sleep(scan_interval * 60)
            
            # å†æ¬¡æ£€æŸ¥é…ç½®ï¼ˆå¯èƒ½åœ¨ç­‰å¾…æœŸé—´è¢«ä¿®æ”¹ï¼‰
            service = get_ai_auto_ban_service()
            if not service.is_enabled() or service.get_scan_interval() <= 0:
                continue

            # æ‰§è¡Œæ‰«æ
            logger.system(f"AI è‡ªåŠ¨å°ç¦: å¼€å§‹å®šæ—¶æ‰«æ (é—´éš”: {scan_interval}åˆ†é’Ÿ)")
            result = await service.run_scan(window="1h", limit=10)

            if result.get("success"):
                stats = result.get("stats", {})
                if stats.get("total_scanned", 0) > 0:
                    logger.business(
                        "AI è‡ªåŠ¨å°ç¦å®šæ—¶æ‰«æå®Œæˆ",
                        scanned=stats.get("total_scanned", 0),
                        banned=stats.get("banned", 0),
                        warned=stats.get("warned", 0),
                        dry_run=result.get("dry_run", True),
                    )

        except asyncio.CancelledError:
            logger.system("AI è‡ªåŠ¨å°ç¦åå°ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.error(f"AI è‡ªåŠ¨å°ç¦åå°ä»»åŠ¡å¼‚å¸¸: {e}", category="ä»»åŠ¡")
            # å‡ºé”™åç­‰å¾… 5 åˆ†é’Ÿå†é‡è¯•
            await asyncio.sleep(300)


async def background_geoip_update():
    """åå°å®šæ—¶æ›´æ–° GeoIP æ•°æ®åº“ï¼ˆæ¯å¤©ä¸€æ¬¡ï¼‰"""
    from .ip_geo_service import update_all_geoip_databases, get_ip_geo_service, GEOIP_UPDATE_INTERVAL

    # å¯åŠ¨åç­‰å¾… 60 ç§’ï¼Œè®©å…¶ä»–æœåŠ¡å…ˆåˆå§‹åŒ–
    await asyncio.sleep(60)
    
    # æ£€æŸ¥å¹¶åˆå§‹åŒ– GeoIP æ•°æ®åº“
    service = get_ip_geo_service()
    if not service.is_available():
        logger.system("[GeoIP] æ•°æ®åº“ä¸å¯ç”¨ï¼Œå°è¯•ä¸‹è½½...")
        try:
            result = await update_all_geoip_databases(force=True)
            if result["success"]:
                logger.system("[GeoIP] æ•°æ®åº“ä¸‹è½½å®Œæˆ")
            else:
                logger.warning(f"[GeoIP] æ•°æ®åº“ä¸‹è½½å¤±è´¥: {result}")
        except Exception as e:
            logger.error(f"[GeoIP] æ•°æ®åº“ä¸‹è½½å¼‚å¸¸: {e}")
    else:
        logger.success("GeoIP æ•°æ®åº“å·²å°±ç»ªï¼Œåå°æ›´æ–°ä»»åŠ¡å·²å¯åŠ¨", category="ä»»åŠ¡")

    while True:
        try:
            # ç­‰å¾…æ›´æ–°é—´éš”ï¼ˆé»˜è®¤ 24 å°æ—¶ï¼‰
            logger.system(f"[GeoIP] ä¸‹æ¬¡æ›´æ–°æ£€æŸ¥åœ¨ {GEOIP_UPDATE_INTERVAL // 3600} å°æ—¶å")
            await asyncio.sleep(GEOIP_UPDATE_INTERVAL)
            
            # æ‰§è¡Œæ›´æ–°
            logger.system("[GeoIP] å¼€å§‹æ£€æŸ¥æ•°æ®åº“æ›´æ–°...")
            result = await update_all_geoip_databases(force=False)
            
            if result["city"]["success"] or result["asn"]["success"]:
                logger.system(
                    f"[GeoIP] æ›´æ–°å®Œæˆ - City: {result['city']['message']}, ASN: {result['asn']['message']}"
                )
            else:
                logger.debug(f"[GeoIP] æ— éœ€æ›´æ–° - {result['city']['message']}, {result['asn']['message']}")

        except asyncio.CancelledError:
            logger.system("[GeoIP] åå°æ›´æ–°ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.error(f"[GeoIP] åå°æ›´æ–°ä»»åŠ¡å¼‚å¸¸: {e}", category="ä»»åŠ¡")
            # å‡ºé”™åç­‰å¾… 1 å°æ—¶å†é‡è¯•
            await asyncio.sleep(3600)


async def background_auto_group_scan():
    """åå°å®šæ—¶æ‰§è¡Œè‡ªåŠ¨åˆ†ç»„æ‰«æ"""
    from .auto_group_service import get_auto_group_service

    # é¢„çƒ­å®Œæˆåç­‰å¾… 2 åˆ†é’Ÿå†å¯åŠ¨
    await asyncio.sleep(120)
    logger.success("è‡ªåŠ¨åˆ†ç»„åå°ä»»åŠ¡å·²å¯åŠ¨", category="ä»»åŠ¡")

    while True:
        try:
            service = get_auto_group_service()

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å®šæ—¶æ‰«æ
            scan_interval = service.get_scan_interval()
            if scan_interval <= 0:
                # å®šæ—¶æ‰«æå·²å…³é—­ï¼Œç­‰å¾… 1 åˆ†é’Ÿåå†æ£€æŸ¥é…ç½®
                await asyncio.sleep(60)
                continue

            # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
            if not service.is_enabled():
                await asyncio.sleep(60)
                continue

            # å…ˆç­‰å¾…é…ç½®çš„æ‰«æé—´éš”ï¼Œå†æ‰§è¡Œæ‰«æ
            logger.system(f"è‡ªåŠ¨åˆ†ç»„: ç­‰å¾… {scan_interval} åˆ†é’Ÿåæ‰§è¡Œå®šæ—¶æ‰«æ")
            await asyncio.sleep(scan_interval * 60)

            # å†æ¬¡æ£€æŸ¥é…ç½®ï¼ˆå¯èƒ½åœ¨ç­‰å¾…æœŸé—´è¢«ä¿®æ”¹ï¼‰
            service = get_auto_group_service()
            if not service.is_enabled() or service.get_scan_interval() <= 0:
                continue

            # æ‰§è¡Œæ‰«æï¼ˆéè¯•è¿è¡Œæ¨¡å¼ï¼‰
            logger.system(f"è‡ªåŠ¨åˆ†ç»„: å¼€å§‹å®šæ—¶æ‰«æ (é—´éš”: {scan_interval}åˆ†é’Ÿ)")
            result = service.run_scan(dry_run=False, operator="system")

            if result.get("success"):
                stats = result.get("stats", {})
                if stats.get("total", 0) > 0:
                    logger.business(
                        "è‡ªåŠ¨åˆ†ç»„å®šæ—¶æ‰«æå®Œæˆ",
                        total=stats.get("total", 0),
                        assigned=stats.get("assigned", 0),
                        skipped=stats.get("skipped", 0),
                        errors=stats.get("errors", 0),
                    )

        except asyncio.CancelledError:
            logger.system("è‡ªåŠ¨åˆ†ç»„åå°ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.error(f"è‡ªåŠ¨åˆ†ç»„åå°ä»»åŠ¡å¼‚å¸¸: {e}", category="ä»»åŠ¡")
            # å‡ºé”™åç­‰å¾… 5 åˆ†é’Ÿå†é‡è¯•
            await asyncio.sleep(300)


# Import routes after app is created to avoid circular imports
def include_routes(app: FastAPI):
    """Include API routes."""
    from .routes import router
    from .auth_routes import router as auth_router
    from .top_up_routes import router as top_up_router
    from .dashboard_routes import router as dashboard_router
    from .storage_routes import router as storage_router
    from .log_analytics_routes import router as analytics_router
    from .user_management_routes import router as user_management_router
    from .risk_monitoring_routes import router as risk_monitoring_router
    from .ip_monitoring_routes import router as ip_monitoring_router
    from .ai_auto_ban_routes import router as ai_auto_ban_router
    from .system_routes import router as system_router
    from .model_status_routes import router as model_status_router
    from .auto_group_routes import router as auto_group_router
    app.include_router(router)
    app.include_router(auth_router)
    app.include_router(top_up_router)
    app.include_router(dashboard_router)
    app.include_router(storage_router)
    app.include_router(analytics_router)
    app.include_router(user_management_router)
    app.include_router(risk_monitoring_router)
    app.include_router(ip_monitoring_router)
    app.include_router(ai_auto_ban_router)
    app.include_router(system_router)
    app.include_router(model_status_router)
    app.include_router(auto_group_router)


# Create FastAPI application
app = FastAPI(
    title="NewAPI Middleware Tool",
    description="API for managing NewAPI redemption codes and database operations",
    version="0.1.0",
    lifespan=lifespan
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Will be configured via environment variable in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include API routes
include_routes(app)


@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log all API requests with timestamp and client information."""
    # Skip logging for health check endpoints
    if request.url.path in ["/api/health", "/api/health/db"]:
        return await call_next(request)

    start_time = time.time()
    client_host = request.client.host if request.client else "unknown"

    response = await call_next(request)

    process_time = time.time() - start_time
    status_code = response.status_code

    # Use the new logger for API requests
    if status_code >= 500:
        logger.api_error(
            request.method,
            request.url.path,
            status_code,
            "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
            client_host
        )
    elif status_code == 401:
        # 401 è®¤è¯å¤±è´¥æ˜¯æ­£å¸¸æµç¨‹ï¼ˆtoken è¿‡æœŸç­‰ï¼‰ï¼Œç”¨ WARN çº§åˆ«
        logger.api_warn(
            request.method,
            request.url.path,
            status_code,
            "è®¤è¯å¤±è´¥",
            client_host
        )
    elif status_code >= 400:
        logger.api_error(
            request.method,
            request.url.path,
            status_code,
            "å®¢æˆ·ç«¯é”™è¯¯",
            client_host
        )
    else:
        logger.api(
            request.method,
            request.url.path,
            status_code,
            process_time,
            client_host
        )

    return response


@app.exception_handler(AppException)
async def app_exception_handler(request: Request, exc: AppException):
    """Handle application-specific exceptions."""
    logger.error(f"åº”ç”¨å¼‚å¸¸: {exc.code} - {exc.message}", category="ç³»ç»Ÿ")
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "success": False,
            "error": {
                "code": exc.code,
                "message": exc.message,
                "details": exc.details
            }
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Handle unexpected exceptions."""
    logger.error(f"æœªé¢„æœŸå¼‚å¸¸: {exc}", category="ç³»ç»Ÿ", exc_info=True)
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "success": False,
            "error": {
                "code": "INTERNAL_ERROR",
                "message": "An unexpected error occurred",
                "details": None
            }
        }
    )


@app.get("/api/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """Health check endpoint."""
    return HealthResponse(status="healthy", version="0.1.0")


@app.get("/api/health/db", tags=["Health"])
async def database_health_check():
    """Database health check endpoint."""
    from .database import get_db_manager
    
    db = get_db_manager()
    try:
        db.connect()
        return {
            "success": True,
            "status": "connected",
            "engine": db.config.engine.value,
            "host": db.config.host,
            "database": db.config.database,
        }
    except DatabaseConnectionError as e:
        return JSONResponse(
            status_code=503,
            content={
                "success": False,
                "status": "disconnected",
                "error": {
                    "code": e.code,
                    "message": e.message,
                    "details": e.details
                }
            }
        )
